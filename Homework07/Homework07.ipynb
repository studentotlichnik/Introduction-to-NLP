{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Homework07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ynl5jECc9b4"
      },
      "source": [
        "# В предыдущей серии\n",
        "\n",
        "\n",
        "<img src=\"images/RNNCompar.png\"/>\n",
        "\n",
        "\n",
        "Мы посмотрели на задачу классификации текстов. Но есть ряд более сильных подходов, которые лучше показывать через задачу генерации\n",
        "\n",
        "\n",
        "# Генерация текстов, encoder-decoder\n",
        "\n",
        "<img src=\"images/EncDec.png\"/>\n",
        "\n",
        "\n",
        "Данная архитектура называется seq2seq, простыми словами выглядит она следующим образом:\n",
        "<img src=\"images/seq2seq.png\"/>\n",
        "\n",
        "\n",
        "эту модель можно строить на уровне слов и на уровне токенов. Попробуем обучить на уровне токенов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZPYjo1Ic9b6"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7n1fxUGc9cA"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 25000\n",
        "data_path = 'data/rus-eng/rus.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6-PIOIFc9cE"
      },
      "source": [
        "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    \n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18sZb83cc9cK"
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50H5TRq2c9cP"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-lW6UmJc9cT"
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGpCwPoOc9cX",
        "outputId": "ee47d955-1b28-4dea-8d68-b8be7589e762",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "for seq_index in range(100):\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.9652 - accuracy: 0.7748 - val_loss: 0.8253 - val_accuracy: 0.7806\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.6887 - accuracy: 0.8176 - val_loss: 0.7416 - val_accuracy: 0.7937\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.5950 - accuracy: 0.8308 - val_loss: 0.6725 - val_accuracy: 0.8082\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.5528 - accuracy: 0.8403 - val_loss: 0.6436 - val_accuracy: 0.8155\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.5290 - accuracy: 0.8460 - val_loss: 0.6186 - val_accuracy: 0.8219\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.5110 - accuracy: 0.8503 - val_loss: 0.6027 - val_accuracy: 0.8263\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.4934 - accuracy: 0.8551 - val_loss: 0.5865 - val_accuracy: 0.8296\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.4769 - accuracy: 0.8600 - val_loss: 0.5739 - val_accuracy: 0.8342\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.4628 - accuracy: 0.8641 - val_loss: 0.5564 - val_accuracy: 0.8382\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.4482 - accuracy: 0.8684 - val_loss: 0.5450 - val_accuracy: 0.8426\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.4354 - accuracy: 0.8722 - val_loss: 0.5328 - val_accuracy: 0.8461\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.4227 - accuracy: 0.8762 - val_loss: 0.5198 - val_accuracy: 0.8499\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.4101 - accuracy: 0.8797 - val_loss: 0.5093 - val_accuracy: 0.8536\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3978 - accuracy: 0.8838 - val_loss: 0.4995 - val_accuracy: 0.8568\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3857 - accuracy: 0.8877 - val_loss: 0.4887 - val_accuracy: 0.8597\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3741 - accuracy: 0.8909 - val_loss: 0.4830 - val_accuracy: 0.8621\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3627 - accuracy: 0.8942 - val_loss: 0.4722 - val_accuracy: 0.8649\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3515 - accuracy: 0.8974 - val_loss: 0.4670 - val_accuracy: 0.8668\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3413 - accuracy: 0.9001 - val_loss: 0.4540 - val_accuracy: 0.8696\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3305 - accuracy: 0.9031 - val_loss: 0.4483 - val_accuracy: 0.8719\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3208 - accuracy: 0.9058 - val_loss: 0.4424 - val_accuracy: 0.8729\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3108 - accuracy: 0.9086 - val_loss: 0.4348 - val_accuracy: 0.8765\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3013 - accuracy: 0.9113 - val_loss: 0.4324 - val_accuracy: 0.8765\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2919 - accuracy: 0.9142 - val_loss: 0.4247 - val_accuracy: 0.8794\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2829 - accuracy: 0.9168 - val_loss: 0.4228 - val_accuracy: 0.8801\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2743 - accuracy: 0.9191 - val_loss: 0.4180 - val_accuracy: 0.8817\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2657 - accuracy: 0.9215 - val_loss: 0.4157 - val_accuracy: 0.8825\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2573 - accuracy: 0.9241 - val_loss: 0.4153 - val_accuracy: 0.8830\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2494 - accuracy: 0.9263 - val_loss: 0.4154 - val_accuracy: 0.8838\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2415 - accuracy: 0.9286 - val_loss: 0.4090 - val_accuracy: 0.8859\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2341 - accuracy: 0.9305 - val_loss: 0.4093 - val_accuracy: 0.8855\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2262 - accuracy: 0.9330 - val_loss: 0.4086 - val_accuracy: 0.8860\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2195 - accuracy: 0.9347 - val_loss: 0.4072 - val_accuracy: 0.8875\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2128 - accuracy: 0.9366 - val_loss: 0.4061 - val_accuracy: 0.8883\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.2057 - accuracy: 0.9387 - val_loss: 0.4073 - val_accuracy: 0.8881\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1988 - accuracy: 0.9405 - val_loss: 0.4092 - val_accuracy: 0.8887\n",
            "Epoch 37/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1924 - accuracy: 0.9425 - val_loss: 0.4118 - val_accuracy: 0.8885\n",
            "Epoch 38/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1863 - accuracy: 0.9441 - val_loss: 0.4130 - val_accuracy: 0.8890\n",
            "Epoch 39/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1801 - accuracy: 0.9461 - val_loss: 0.4141 - val_accuracy: 0.8892\n",
            "Epoch 40/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1743 - accuracy: 0.9476 - val_loss: 0.4173 - val_accuracy: 0.8890\n",
            "Epoch 41/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1690 - accuracy: 0.9490 - val_loss: 0.4213 - val_accuracy: 0.8896\n",
            "Epoch 42/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1639 - accuracy: 0.9505 - val_loss: 0.4261 - val_accuracy: 0.8886\n",
            "Epoch 43/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1586 - accuracy: 0.9522 - val_loss: 0.4268 - val_accuracy: 0.8886\n",
            "Epoch 44/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1531 - accuracy: 0.9535 - val_loss: 0.4322 - val_accuracy: 0.8890\n",
            "Epoch 45/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1484 - accuracy: 0.9550 - val_loss: 0.4338 - val_accuracy: 0.8890\n",
            "Epoch 46/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1442 - accuracy: 0.9561 - val_loss: 0.4390 - val_accuracy: 0.8888\n",
            "Epoch 47/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1391 - accuracy: 0.9576 - val_loss: 0.4412 - val_accuracy: 0.8892\n",
            "Epoch 48/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1344 - accuracy: 0.9591 - val_loss: 0.4516 - val_accuracy: 0.8879\n",
            "Epoch 49/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1301 - accuracy: 0.9603 - val_loss: 0.4536 - val_accuracy: 0.8884\n",
            "Epoch 50/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1262 - accuracy: 0.9615 - val_loss: 0.4600 - val_accuracy: 0.8876\n",
            "Epoch 51/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1227 - accuracy: 0.9623 - val_loss: 0.4634 - val_accuracy: 0.8880\n",
            "Epoch 52/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1184 - accuracy: 0.9635 - val_loss: 0.4714 - val_accuracy: 0.8869\n",
            "Epoch 53/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1147 - accuracy: 0.9645 - val_loss: 0.4747 - val_accuracy: 0.8875\n",
            "Epoch 54/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1111 - accuracy: 0.9658 - val_loss: 0.4800 - val_accuracy: 0.8867\n",
            "Epoch 55/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1079 - accuracy: 0.9666 - val_loss: 0.4861 - val_accuracy: 0.8862\n",
            "Epoch 56/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1053 - accuracy: 0.9674 - val_loss: 0.4901 - val_accuracy: 0.8862\n",
            "Epoch 57/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1022 - accuracy: 0.9681 - val_loss: 0.4977 - val_accuracy: 0.8863\n",
            "Epoch 58/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0980 - accuracy: 0.9694 - val_loss: 0.5020 - val_accuracy: 0.8863\n",
            "Epoch 59/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0954 - accuracy: 0.9700 - val_loss: 0.5062 - val_accuracy: 0.8855\n",
            "Epoch 60/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0923 - accuracy: 0.9709 - val_loss: 0.5118 - val_accuracy: 0.8866\n",
            "Epoch 61/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0898 - accuracy: 0.9718 - val_loss: 0.5286 - val_accuracy: 0.8833\n",
            "Epoch 62/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0874 - accuracy: 0.9723 - val_loss: 0.5262 - val_accuracy: 0.8852\n",
            "Epoch 63/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0845 - accuracy: 0.9731 - val_loss: 0.5350 - val_accuracy: 0.8842\n",
            "Epoch 64/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0824 - accuracy: 0.9737 - val_loss: 0.5350 - val_accuracy: 0.8856\n",
            "Epoch 65/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0805 - accuracy: 0.9744 - val_loss: 0.5437 - val_accuracy: 0.8846\n",
            "Epoch 66/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0781 - accuracy: 0.9749 - val_loss: 0.5490 - val_accuracy: 0.8838\n",
            "Epoch 67/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0768 - accuracy: 0.9750 - val_loss: 0.5528 - val_accuracy: 0.8843\n",
            "Epoch 68/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0742 - accuracy: 0.9760 - val_loss: 0.5589 - val_accuracy: 0.8839\n",
            "Epoch 69/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0723 - accuracy: 0.9766 - val_loss: 0.5699 - val_accuracy: 0.8829\n",
            "Epoch 70/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0700 - accuracy: 0.9772 - val_loss: 0.5750 - val_accuracy: 0.8831\n",
            "Epoch 71/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0684 - accuracy: 0.9777 - val_loss: 0.5758 - val_accuracy: 0.8834\n",
            "Epoch 72/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0670 - accuracy: 0.9781 - val_loss: 0.5878 - val_accuracy: 0.8831\n",
            "Epoch 73/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0651 - accuracy: 0.9786 - val_loss: 0.5913 - val_accuracy: 0.8822\n",
            "Epoch 74/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0643 - accuracy: 0.9787 - val_loss: 0.5923 - val_accuracy: 0.8829\n",
            "Epoch 75/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0627 - accuracy: 0.9793 - val_loss: 0.6006 - val_accuracy: 0.8825\n",
            "Epoch 76/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0605 - accuracy: 0.9799 - val_loss: 0.6071 - val_accuracy: 0.8818\n",
            "Epoch 77/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0592 - accuracy: 0.9804 - val_loss: 0.6102 - val_accuracy: 0.8822\n",
            "Epoch 78/100\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.0584 - accuracy: 0.9806 - val_loss: 0.6178 - val_accuracy: 0.8814\n",
            "Epoch 79/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0576 - accuracy: 0.9806 - val_loss: 0.6195 - val_accuracy: 0.8822\n",
            "Epoch 80/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0559 - accuracy: 0.9812 - val_loss: 0.6239 - val_accuracy: 0.8819\n",
            "Epoch 81/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0550 - accuracy: 0.9814 - val_loss: 0.6335 - val_accuracy: 0.8813\n",
            "Epoch 82/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0544 - accuracy: 0.9816 - val_loss: 0.6349 - val_accuracy: 0.8814\n",
            "Epoch 83/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0530 - accuracy: 0.9820 - val_loss: 0.6404 - val_accuracy: 0.8813\n",
            "Epoch 84/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0527 - accuracy: 0.9821 - val_loss: 0.6464 - val_accuracy: 0.8807\n",
            "Epoch 85/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0515 - accuracy: 0.9823 - val_loss: 0.6520 - val_accuracy: 0.8813\n",
            "Epoch 86/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0504 - accuracy: 0.9828 - val_loss: 0.6516 - val_accuracy: 0.8819\n",
            "Epoch 87/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0493 - accuracy: 0.9830 - val_loss: 0.6540 - val_accuracy: 0.8814\n",
            "Epoch 88/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0490 - accuracy: 0.9831 - val_loss: 0.6630 - val_accuracy: 0.8816\n",
            "Epoch 89/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0477 - accuracy: 0.9836 - val_loss: 0.6650 - val_accuracy: 0.8809\n",
            "Epoch 90/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0470 - accuracy: 0.9838 - val_loss: 0.6744 - val_accuracy: 0.8800\n",
            "Epoch 91/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0471 - accuracy: 0.9838 - val_loss: 0.6754 - val_accuracy: 0.8806\n",
            "Epoch 92/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0480 - accuracy: 0.9832 - val_loss: 0.6694 - val_accuracy: 0.8814\n",
            "Epoch 93/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0467 - accuracy: 0.9836 - val_loss: 0.6753 - val_accuracy: 0.8808\n",
            "Epoch 94/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0449 - accuracy: 0.9842 - val_loss: 0.6843 - val_accuracy: 0.8803\n",
            "Epoch 95/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0441 - accuracy: 0.9844 - val_loss: 0.6846 - val_accuracy: 0.8805\n",
            "Epoch 96/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0437 - accuracy: 0.9844 - val_loss: 0.6918 - val_accuracy: 0.8804\n",
            "Epoch 97/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0429 - accuracy: 0.9847 - val_loss: 0.6950 - val_accuracy: 0.8802\n",
            "Epoch 98/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0426 - accuracy: 0.9847 - val_loss: 0.7010 - val_accuracy: 0.8799\n",
            "Epoch 99/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0421 - accuracy: 0.9849 - val_loss: 0.7047 - val_accuracy: 0.8797\n",
            "Epoch 100/100\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0418 - accuracy: 0.9851 - val_loss: 0.7103 - val_accuracy: 0.8792\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Здравствуйте.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Беги!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Беги!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Беги!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Беги!\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Кто?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Огонь!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Огонь!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Спасите!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Спасите!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Спасите!\n",
            "\n",
            "-\n",
            "Input sentence: Hide.\n",
            "Decoded sentence: Прячьтесь.\n",
            "\n",
            "-\n",
            "Input sentence: Hide.\n",
            "Decoded sentence: Прячьтесь.\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Прыгай!\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Прыгай!\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Прыгай!\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Прыгай!\n",
            "\n",
            "-\n",
            "Input sentence: Stay.\n",
            "Decoded sentence: Оставайтесь.\n",
            "\n",
            "-\n",
            "Input sentence: Stay.\n",
            "Decoded sentence: Оставайтесь.\n",
            "\n",
            "-\n",
            "Input sentence: Stay.\n",
            "Decoded sentence: Оставайтесь.\n",
            "\n",
            "-\n",
            "Input sentence: Stay.\n",
            "Decoded sentence: Оставайтесь.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Остановитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Остановитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Остановитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Жди!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Жди!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Жди!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Жди!\n",
            "\n",
            "-\n",
            "Input sentence: Wait.\n",
            "Decoded sentence: Ждите.\n",
            "\n",
            "-\n",
            "Input sentence: Wait.\n",
            "Decoded sentence: Ждите.\n",
            "\n",
            "-\n",
            "Input sentence: Wait.\n",
            "Decoded sentence: Ждите.\n",
            "\n",
            "-\n",
            "Input sentence: Do it.\n",
            "Decoded sentence: Сделай это.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Продолжай.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Продолжай.\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Привет!\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Привет!\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Привет!\n",
            "\n",
            "-\n",
            "Input sentence: Hurry!\n",
            "Decoded sentence: Поспешите.\n",
            "\n",
            "-\n",
            "Input sentence: I ran.\n",
            "Decoded sentence: Я побежала.\n",
            "\n",
            "-\n",
            "Input sentence: I ran.\n",
            "Decoded sentence: Я побежала.\n",
            "\n",
            "-\n",
            "Input sentence: I ran.\n",
            "Decoded sentence: Я побежала.\n",
            "\n",
            "-\n",
            "Input sentence: I ran.\n",
            "Decoded sentence: Я побежала.\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Понимаю.\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Понимаю.\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Понимаю.\n",
            "\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: Я пытаюсь.\n",
            "\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: Я пытаюсь.\n",
            "\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: Я пытаюсь.\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Я победил!\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Я победил!\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Я победил!\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Я победил!\n",
            "\n",
            "-\n",
            "Input sentence: Oh no!\n",
            "Decoded sentence: О нет!\n",
            "\n",
            "-\n",
            "Input sentence: Relax.\n",
            "Decoded sentence: Попустись.\n",
            "\n",
            "-\n",
            "Input sentence: Relax.\n",
            "Decoded sentence: Попустись.\n",
            "\n",
            "-\n",
            "Input sentence: Relax.\n",
            "Decoded sentence: Попустись.\n",
            "\n",
            "-\n",
            "Input sentence: Shoot!\n",
            "Decoded sentence: Стажай!\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Attack!\n",
            "Decoded sentence: В анав!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За Ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За Ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За Ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За Ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За Ваше здоровье!\n",
            "\n",
            "-\n",
            "Input sentence: Eat it.\n",
            "Decoded sentence: Съешь это.\n",
            "\n",
            "-\n",
            "Input sentence: Eat up.\n",
            "Decoded sentence: Доедай.\n",
            "\n",
            "-\n",
            "Input sentence: Freeze!\n",
            "Decoded sentence: Никому не двигаться.\n",
            "\n",
            "-\n",
            "Input sentence: Freeze!\n",
            "Decoded sentence: Никому не двигаться.\n",
            "\n",
            "-\n",
            "Input sentence: Freeze!\n",
            "Decoded sentence: Никому не двигаться.\n",
            "\n",
            "-\n",
            "Input sentence: Freeze!\n",
            "Decoded sentence: Никому не двигаться.\n",
            "\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Иди сейчас.\n",
            "\n",
            "CPU times: user 7min 25s, sys: 46.7 s, total: 8min 12s\n",
            "Wall time: 7min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB_FwJ_tc9ce"
      },
      "source": [
        "Но есть проблемы:\n",
        "- на длинных последовательностях результат будет не очень - быстро забывается контекст\n",
        "- хочется научить сеть смотреть в определенное место в прошлом при генерации\n",
        "\n",
        "attention\n",
        "\n",
        "<img src=\"images/Attention.png\"/>\n",
        "\n",
        "<img src=\"images/Attention2.png\"/>\n",
        "\n",
        "\n",
        "- h(t): скрытое состояние декодера\n",
        "- c(t): вектор контекста, который подается на вход\n",
        "- y(t): текущий таргет\n",
        "- $\\bar{h}(t)$: скрытое состояние attention\n",
        "- a(t): скор нормализации\n",
        "\n",
        "\n",
        "$$\\bar{h}(t)\\ =\\ tanh(W_c\\ [c_t,\\ h_t]) $$\n",
        "\n",
        "$$P(y_t|y_{<t},\\ x)\\ =\\ softmax(W_s\\ \\bar{h}_t) $$\n",
        "\n",
        "\n",
        "Зачем нужен скор нормализации? - пытаемся сравнить похожесть текущего скрытого состояния и скрытого состояния из прошлого и понять, на что обращать внимание\n",
        "\n",
        "\n",
        "$$a_t(s)\\ =\\ \\frac{exp(score(h_t,\\ \\bar{h}_s))}{\\sum_{i}\\ exp(score(h_t,\\ \\bar{h}_i)) }$$ \n",
        "\n",
        "\n",
        "<img src=\"images/scores.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Ii7ohUc9cf"
      },
      "source": [
        "## Поднимемся на уровень слов, чтобы можно было что-то более адекватное посчитать за адекватное время"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEJPa13pc9cg"
      },
      "source": [
        "import re\n",
        "import tensorflow.compat.v1 as tf\n",
        "data_path = 'data/rus-eng/rus.txt'\n",
        "num_samples = 25000\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(preprocess_sentence(input_text))\n",
        "    target_texts.append(preprocess_sentence(target_text))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2MRFdLfc9ck"
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P25UI3mDc9cn"
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
        "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV3oMM6Oc9cq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WzPogGlc9cv"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK_YIl99c9cy"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.lstm(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "    \n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "    \n",
        "    \n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.lstm(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PabPBBVic9c1"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24fjCYFGc9c4"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    \n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDTqHvPEc9c7",
        "outputId": "ca217399-cea1-4b15-ac81-26c628de1fd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "EPOCHS = 100\n",
        "for epoch in range(EPOCHS):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.0651\n",
            "Epoch 2 Loss 0.0238\n",
            "Epoch 3 Loss 0.0199\n",
            "Epoch 4 Loss 0.0171\n",
            "Epoch 5 Loss 0.0163\n",
            "Epoch 6 Loss 0.0153\n",
            "Epoch 7 Loss 0.0138\n",
            "Epoch 8 Loss 0.0129\n",
            "Epoch 9 Loss 0.0127\n",
            "Epoch 10 Loss 0.0118\n",
            "Epoch 11 Loss 0.0113\n",
            "Epoch 12 Loss 0.0107\n",
            "Epoch 13 Loss 0.0104\n",
            "Epoch 14 Loss 0.0094\n",
            "Epoch 15 Loss 0.0102\n",
            "Epoch 16 Loss 0.0094\n",
            "Epoch 17 Loss 0.0087\n",
            "Epoch 18 Loss 0.0082\n",
            "Epoch 19 Loss 0.0087\n",
            "Epoch 20 Loss 0.0077\n",
            "Epoch 21 Loss 0.0074\n",
            "Epoch 22 Loss 0.0078\n",
            "Epoch 23 Loss 0.0069\n",
            "Epoch 24 Loss 0.0061\n",
            "Epoch 25 Loss 0.0062\n",
            "Epoch 26 Loss 0.0066\n",
            "Epoch 27 Loss 0.0069\n",
            "Epoch 28 Loss 0.0069\n",
            "Epoch 29 Loss 0.0059\n",
            "Epoch 30 Loss 0.0055\n",
            "Epoch 31 Loss 0.0055\n",
            "Epoch 32 Loss 0.0057\n",
            "Epoch 33 Loss 0.0054\n",
            "Epoch 34 Loss 0.0050\n",
            "Epoch 35 Loss 0.0048\n",
            "Epoch 36 Loss 0.0045\n",
            "Epoch 37 Loss 0.0046\n",
            "Epoch 38 Loss 0.0046\n",
            "Epoch 39 Loss 0.0068\n",
            "Epoch 40 Loss 0.0062\n",
            "Epoch 41 Loss 0.0060\n",
            "Epoch 42 Loss 0.0052\n",
            "Epoch 43 Loss 0.0056\n",
            "Epoch 44 Loss 0.0052\n",
            "Epoch 45 Loss 0.0049\n",
            "Epoch 46 Loss 0.0043\n",
            "Epoch 47 Loss 0.0042\n",
            "Epoch 48 Loss 0.0040\n",
            "Epoch 49 Loss 0.0041\n",
            "Epoch 50 Loss 0.0042\n",
            "Epoch 51 Loss 0.0046\n",
            "Epoch 52 Loss 0.0053\n",
            "Epoch 53 Loss 0.0051\n",
            "Epoch 54 Loss 0.0053\n",
            "Epoch 55 Loss 0.0051\n",
            "Epoch 56 Loss 0.0049\n",
            "Epoch 57 Loss 0.0047\n",
            "Epoch 58 Loss 0.0050\n",
            "Epoch 59 Loss 0.0042\n",
            "Epoch 60 Loss 0.0040\n",
            "Epoch 61 Loss 0.0039\n",
            "Epoch 62 Loss 0.0039\n",
            "Epoch 63 Loss 0.0039\n",
            "Epoch 64 Loss 0.0038\n",
            "Epoch 65 Loss 0.0039\n",
            "Epoch 66 Loss 0.0039\n",
            "Epoch 67 Loss 0.0040\n",
            "Epoch 68 Loss 0.0040\n",
            "Epoch 69 Loss 0.0050\n",
            "Epoch 70 Loss 0.0061\n",
            "Epoch 71 Loss 0.0049\n",
            "Epoch 72 Loss 0.0049\n",
            "Epoch 73 Loss 0.0042\n",
            "Epoch 74 Loss 0.0041\n",
            "Epoch 75 Loss 0.0039\n",
            "Epoch 76 Loss 0.0038\n",
            "Epoch 77 Loss 0.0038\n",
            "Epoch 78 Loss 0.0039\n",
            "Epoch 79 Loss 0.0039\n",
            "Epoch 80 Loss 0.0039\n",
            "Epoch 81 Loss 0.0042\n",
            "Epoch 82 Loss 0.0050\n",
            "Epoch 83 Loss 0.0051\n",
            "Epoch 84 Loss 0.0054\n",
            "Epoch 85 Loss 0.0041\n",
            "Epoch 86 Loss 0.0043\n",
            "Epoch 87 Loss 0.0041\n",
            "Epoch 88 Loss 0.0040\n",
            "Epoch 89 Loss 0.0038\n",
            "Epoch 90 Loss 0.0038\n",
            "Epoch 91 Loss 0.0037\n",
            "Epoch 92 Loss 0.0039\n",
            "Epoch 93 Loss 0.0043\n",
            "Epoch 94 Loss 0.0043\n",
            "Epoch 95 Loss 0.0041\n",
            "Epoch 96 Loss 0.0047\n",
            "Epoch 97 Loss 0.0043\n",
            "Epoch 98 Loss 0.0048\n",
            "Epoch 99 Loss 0.0045\n",
            "Epoch 100 Loss 0.0045\n",
            "CPU times: user 19min 11s, sys: 1min 35s, total: 20min 46s\n",
            "Wall time: 21min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65PD0NfUc9c-"
      },
      "source": [
        "Некоторые украденные функции для оценки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lndb1Wwc9c-"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "    #ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    #ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "    \n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PScQtuOgc9dB",
        "outputId": "eb2db466-95a0-4e3c-e298-1eaf02e8ea54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        }
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "translate(u'good morning')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "Input: <start> good morning <end>\n",
            "Predicted translation: . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['char', 'f']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBxK_vDHftxl",
        "outputId": "4fbabb6f-c371-4ab2-d2e1-6e58384cb94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "translate(u'how are you')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> how are you <end>\n",
            "Predicted translation: ! <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Bnfl6Cfzpr",
        "outputId": "30789c63-af6f-49bf-c877-81a9c92e6feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "translate(u'you are my best friend')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> you are my best friend <end>\n",
            "Predicted translation: , . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D_vmVw3c9dE"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "<img src=\"images/transformer.png\"/>\n",
        "\n",
        "\n",
        "Идея в том, что каждое слово параллельно проходит через слои, изображенные на картинке.\n",
        "Некоторые из них — это стандартные fully-connected layers, некоторые — shortcut connections как в ResNet (там, где на картинке Add).\n",
        "\n",
        "\n",
        "Multi-head attention - это специальный новый слой, который дает возможность каждому входному вектору взаимодействовать с другими словами через attention mechanism, вместо передачи hidden state как в RNN или соседних слов как в CNN.\n",
        "\n",
        "\n",
        "<img src=\"images/mha.png\"/>\n",
        "\n",
        "\n",
        "<img src=\"images/AttTr.png\"/>\n",
        "\n",
        "\n",
        "Работа энкодера:\n",
        "\n",
        "\n",
        "Делаются эмбеддинги для всех слов предложения (вектора одинаковой размерности). Для примера пусть это будет предложение I am stupid. В эмбеддинг добавляется еще позиция слова в предложении.\n",
        "\n",
        "\n",
        "Берется вектор первого слова и вектор второго слова (I, am), подаются на однослойную сеть с одним выходом, которая выдает степень их похожести (скалярная величина). Эта скалярная величина умножается на вектор второго слова, получая его некоторую \"ослабленную\" на величину похожести копию.\n",
        "\n",
        "\n",
        "Вместо второго слова подается третье слово и делается тоже самое что в п.2. с той же самой сетью с теми же весами (для векторов I, stupid).\n",
        "\n",
        "\n",
        "Делая тоже самое для всех оставшихся слов предложения получаются их \"ослабленные\" (взвешенные) копии, которые выражают степень их похожести на первое слово. Далее эти все взвешенные вектора складываются друг с другом, получая один результирующий вектор размерности одного эмбединга:\n",
        "output=am * weight(I, am) + stupid * weight(I, stupid)\n",
        "\n",
        "\n",
        "Это механизм \"обычного\" attention.\n",
        "Так как оценка похожести слов всего одним способом (по одному критерию) считается недостаточной, тоже самое (п.2-4) повторяется несколько раз с другими весами. Типа одна один attention может определять похожесть слов по смысловой нагрузке, другой по грамматической, остальные еще как-то и т.п.\n",
        "\n",
        "\n",
        "На выходе п.5. получается несколько векторов, каждый из которых является взвешенной суммой всех остальных слов предложения относительно их похожести на первое слово (I). Конкантенируем этот вректор в один.\n",
        "\n",
        "\n",
        "Дальше ставится еще один слой линейного преобразования, уменьшающий размерность результата п.6. до размерности вектора одного эмбединга. Получается некое представление первого слова предложения, составленное из взвешенных векторов всех остальных слов предложения.\n",
        "\n",
        "\n",
        "Такой же процесс производится для всех других слов в предложении.\n",
        "\n",
        "\n",
        "Так как размерность выхода та же, то можно проделать все тоже самое еще раз (п.2-8), но вместо оригинальных эмбеддингов слов взять то, что получается после прохода через этот Multi-head attention, а нейросети аттеншенов внутри взять с другими весами (веса между слоями не общие). И таких слоев можно сделать много (у гугла 6). Однако между первым и вторым слоем добавляется еще полносвязный слой и residual соединения, чтобы добавить сети выразительности."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ATZClhsc9dF"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h697jl3Ec9dI"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50C9F499c9dK"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "    \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "    \n",
        "    \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0JUbjtc9dN"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0kDO_DVc9dQ"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNrh-8zjc9dU"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
        "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L43Qnij1c9dX"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tx7mWi8c9da"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "  \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qTbmYWfc9dd"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I47VoYdwc9df"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTFb65CRc9di"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtcuDDQwc9dk"
      },
      "source": [
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "  \n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K_91dy4c9dn"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_iBTwfmgSDE",
        "outputId": "cd02d938-25d1-44b8-8a72-19067adfe1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "EPOCHS = 100\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.0280 Accuracy 0.0156\n",
            "Epoch 1 Batch 50 Loss 1.9406 Accuracy 0.1394\n",
            "Epoch 1 Batch 100 Loss 1.5055 Accuracy 0.2006\n",
            "Epoch 1 Batch 150 Loss 1.2109 Accuracy 0.2687\n",
            "Epoch 1 Batch 200 Loss 0.9597 Accuracy 0.3223\n",
            "Epoch 1 Batch 250 Loss 0.7981 Accuracy 0.3561\n",
            "Epoch 1 Batch 300 Loss 0.6866 Accuracy 0.3789\n",
            "Epoch 1 Loss 0.6659 Accuracy 0.3830\n",
            "Epoch 2 Batch 0 Loss 0.2557 Accuracy 0.4805\n",
            "Epoch 2 Batch 50 Loss 0.1100 Accuracy 0.4947\n",
            "Epoch 2 Batch 100 Loss 0.1128 Accuracy 0.4941\n",
            "Epoch 2 Batch 150 Loss 0.1081 Accuracy 0.4944\n",
            "Epoch 2 Batch 200 Loss 0.1047 Accuracy 0.4944\n",
            "Epoch 2 Batch 250 Loss 0.1028 Accuracy 0.4946\n",
            "Epoch 2 Batch 300 Loss 0.1000 Accuracy 0.4949\n",
            "Epoch 2 Loss 0.0996 Accuracy 0.4950\n",
            "Epoch 3 Batch 0 Loss 0.1919 Accuracy 0.5000\n",
            "Epoch 3 Batch 50 Loss 0.0913 Accuracy 0.4972\n",
            "Epoch 3 Batch 100 Loss 0.0809 Accuracy 0.4973\n",
            "Epoch 3 Batch 150 Loss 0.0780 Accuracy 0.4973\n",
            "Epoch 3 Batch 200 Loss 0.0768 Accuracy 0.4975\n",
            "Epoch 3 Batch 250 Loss 0.0752 Accuracy 0.4974\n",
            "Epoch 3 Batch 300 Loss 0.0740 Accuracy 0.4977\n",
            "Epoch 3 Loss 0.0733 Accuracy 0.4978\n",
            "Epoch 4 Batch 0 Loss 0.1268 Accuracy 0.4961\n",
            "Epoch 4 Batch 50 Loss 0.0628 Accuracy 0.4988\n",
            "Epoch 4 Batch 100 Loss 0.0724 Accuracy 0.4970\n",
            "Epoch 4 Batch 150 Loss 0.0715 Accuracy 0.4975\n",
            "Epoch 4 Batch 200 Loss 0.0685 Accuracy 0.4980\n",
            "Epoch 4 Batch 250 Loss 0.0696 Accuracy 0.4981\n",
            "Epoch 4 Batch 300 Loss 0.0682 Accuracy 0.4985\n",
            "Epoch 4 Loss 0.0675 Accuracy 0.4986\n",
            "Epoch 5 Batch 0 Loss 0.0445 Accuracy 0.5000\n",
            "Epoch 5 Batch 50 Loss 0.0568 Accuracy 0.4995\n",
            "Epoch 5 Batch 100 Loss 0.0596 Accuracy 0.4993\n",
            "Epoch 5 Batch 150 Loss 0.0580 Accuracy 0.4993\n",
            "Epoch 5 Batch 200 Loss 0.0581 Accuracy 0.4993\n",
            "Epoch 5 Batch 250 Loss 0.0598 Accuracy 0.4995\n",
            "Epoch 5 Batch 300 Loss 0.0607 Accuracy 0.4995\n",
            "Epoch 5 Loss 0.0610 Accuracy 0.4995\n",
            "Epoch 6 Batch 0 Loss 0.1372 Accuracy 0.5039\n",
            "Epoch 6 Batch 50 Loss 0.0550 Accuracy 0.4996\n",
            "Epoch 6 Batch 100 Loss 0.0546 Accuracy 0.5001\n",
            "Epoch 6 Batch 150 Loss 0.0576 Accuracy 0.4996\n",
            "Epoch 6 Batch 200 Loss 0.0588 Accuracy 0.4995\n",
            "Epoch 6 Batch 250 Loss 0.0611 Accuracy 0.4995\n",
            "Epoch 6 Batch 300 Loss 0.0608 Accuracy 0.4996\n",
            "Epoch 6 Loss 0.0603 Accuracy 0.4996\n",
            "Epoch 7 Batch 0 Loss 0.0145 Accuracy 0.4961\n",
            "Epoch 7 Batch 50 Loss 0.0666 Accuracy 0.4984\n",
            "Epoch 7 Batch 100 Loss 0.0609 Accuracy 0.4991\n",
            "Epoch 7 Batch 150 Loss 0.0585 Accuracy 0.4993\n",
            "Epoch 7 Batch 200 Loss 0.0564 Accuracy 0.4994\n",
            "Epoch 7 Batch 250 Loss 0.0592 Accuracy 0.4994\n",
            "Epoch 7 Batch 300 Loss 0.0567 Accuracy 0.4997\n",
            "Epoch 7 Loss 0.0566 Accuracy 0.4997\n",
            "Epoch 8 Batch 0 Loss 0.0934 Accuracy 0.4961\n",
            "Epoch 8 Batch 50 Loss 0.0548 Accuracy 0.4995\n",
            "Epoch 8 Batch 100 Loss 0.0555 Accuracy 0.5004\n",
            "Epoch 8 Batch 150 Loss 0.0589 Accuracy 0.4998\n",
            "Epoch 8 Batch 200 Loss 0.0586 Accuracy 0.4990\n",
            "Epoch 8 Batch 250 Loss 0.0587 Accuracy 0.4993\n",
            "Epoch 8 Batch 300 Loss 0.0576 Accuracy 0.4995\n",
            "Epoch 8 Loss 0.0573 Accuracy 0.4995\n",
            "Epoch 9 Batch 0 Loss 0.0368 Accuracy 0.5000\n",
            "Epoch 9 Batch 50 Loss 0.0522 Accuracy 0.4998\n",
            "Epoch 9 Batch 100 Loss 0.0534 Accuracy 0.5004\n",
            "Epoch 9 Batch 150 Loss 0.0543 Accuracy 0.5004\n",
            "Epoch 9 Batch 200 Loss 0.0535 Accuracy 0.5001\n",
            "Epoch 9 Batch 250 Loss 0.0570 Accuracy 0.4999\n",
            "Epoch 9 Batch 300 Loss 0.0554 Accuracy 0.5002\n",
            "Epoch 9 Loss 0.0553 Accuracy 0.5002\n",
            "Epoch 10 Batch 0 Loss 0.0565 Accuracy 0.4961\n",
            "Epoch 10 Batch 50 Loss 0.0537 Accuracy 0.4990\n",
            "Epoch 10 Batch 100 Loss 0.0519 Accuracy 0.5003\n",
            "Epoch 10 Batch 150 Loss 0.0620 Accuracy 0.4990\n",
            "Epoch 10 Batch 200 Loss 0.0655 Accuracy 0.4982\n",
            "Epoch 10 Batch 250 Loss 0.0633 Accuracy 0.4983\n",
            "Epoch 10 Batch 300 Loss 0.0617 Accuracy 0.4986\n",
            "Epoch 10 Loss 0.0621 Accuracy 0.4986\n",
            "Epoch 11 Batch 0 Loss 0.0275 Accuracy 0.5039\n",
            "Epoch 11 Batch 50 Loss 0.0616 Accuracy 0.4997\n",
            "Epoch 11 Batch 100 Loss 0.0609 Accuracy 0.4991\n",
            "Epoch 11 Batch 150 Loss 0.0599 Accuracy 0.4993\n",
            "Epoch 11 Batch 200 Loss 0.0595 Accuracy 0.4994\n",
            "Epoch 11 Batch 250 Loss 0.0668 Accuracy 0.4992\n",
            "Epoch 11 Batch 300 Loss 0.0659 Accuracy 0.4993\n",
            "Epoch 11 Loss 0.0661 Accuracy 0.4992\n",
            "Epoch 12 Batch 0 Loss 0.0360 Accuracy 0.5039\n",
            "Epoch 12 Batch 50 Loss 0.0574 Accuracy 0.4995\n",
            "Epoch 12 Batch 100 Loss 0.0551 Accuracy 0.4998\n",
            "Epoch 12 Batch 150 Loss 0.0604 Accuracy 0.4993\n",
            "Epoch 12 Batch 200 Loss 0.0612 Accuracy 0.4998\n",
            "Epoch 12 Batch 250 Loss 0.0606 Accuracy 0.4999\n",
            "Epoch 12 Batch 300 Loss 0.0581 Accuracy 0.5000\n",
            "Epoch 12 Loss 0.0580 Accuracy 0.5000\n",
            "Epoch 13 Batch 0 Loss 0.0539 Accuracy 0.5000\n",
            "Epoch 13 Batch 50 Loss 0.0711 Accuracy 0.5002\n",
            "Epoch 13 Batch 100 Loss 0.0711 Accuracy 0.4992\n",
            "Epoch 13 Batch 150 Loss 0.0675 Accuracy 0.4999\n",
            "Epoch 13 Batch 200 Loss 0.0644 Accuracy 0.5000\n",
            "Epoch 13 Batch 250 Loss 0.0650 Accuracy 0.4996\n",
            "Epoch 13 Batch 300 Loss 0.0648 Accuracy 0.4994\n",
            "Epoch 13 Loss 0.0649 Accuracy 0.4993\n",
            "Epoch 14 Batch 0 Loss 0.1728 Accuracy 0.4922\n",
            "Epoch 14 Batch 50 Loss 0.0591 Accuracy 0.4999\n",
            "Epoch 14 Batch 100 Loss 0.0544 Accuracy 0.5003\n",
            "Epoch 14 Batch 150 Loss 0.0536 Accuracy 0.5004\n",
            "Epoch 14 Batch 200 Loss 0.0529 Accuracy 0.5009\n",
            "Epoch 14 Batch 250 Loss 0.0531 Accuracy 0.5008\n",
            "Epoch 14 Batch 300 Loss 0.0541 Accuracy 0.5007\n",
            "Epoch 14 Loss 0.0542 Accuracy 0.5006\n",
            "Epoch 15 Batch 0 Loss 0.0248 Accuracy 0.5000\n",
            "Epoch 15 Batch 50 Loss 0.0545 Accuracy 0.4997\n",
            "Epoch 15 Batch 100 Loss 0.0630 Accuracy 0.5001\n",
            "Epoch 15 Batch 150 Loss 0.0579 Accuracy 0.5004\n",
            "Epoch 15 Batch 200 Loss 0.0562 Accuracy 0.5005\n",
            "Epoch 15 Batch 250 Loss 0.0577 Accuracy 0.5004\n",
            "Epoch 15 Batch 300 Loss 0.0601 Accuracy 0.5001\n",
            "Epoch 15 Loss 0.0605 Accuracy 0.5000\n",
            "Epoch 16 Batch 0 Loss 0.1619 Accuracy 0.4883\n",
            "Epoch 16 Batch 50 Loss 0.0589 Accuracy 0.4964\n",
            "Epoch 16 Batch 100 Loss 0.0636 Accuracy 0.4966\n",
            "Epoch 16 Batch 150 Loss 0.0650 Accuracy 0.4963\n",
            "Epoch 16 Batch 200 Loss 0.0686 Accuracy 0.4960\n",
            "Epoch 16 Batch 250 Loss 0.0750 Accuracy 0.4955\n",
            "Epoch 16 Batch 300 Loss 0.1724 Accuracy 0.4716\n",
            "Epoch 16 Loss 0.1965 Accuracy 0.4636\n",
            "Epoch 17 Batch 0 Loss 0.8534 Accuracy 0.2539\n",
            "Epoch 17 Batch 50 Loss 0.8387 Accuracy 0.2446\n",
            "Epoch 17 Batch 100 Loss 0.8477 Accuracy 0.2444\n",
            "Epoch 17 Batch 150 Loss 0.8502 Accuracy 0.2436\n",
            "Epoch 17 Batch 200 Loss 0.8463 Accuracy 0.2430\n",
            "Epoch 17 Batch 250 Loss 0.8471 Accuracy 0.2447\n",
            "Epoch 17 Batch 300 Loss 0.8479 Accuracy 0.2453\n",
            "Epoch 17 Loss 0.8479 Accuracy 0.2453\n",
            "Epoch 18 Batch 0 Loss 0.9458 Accuracy 0.2617\n",
            "Epoch 18 Batch 50 Loss 0.8454 Accuracy 0.2607\n",
            "Epoch 18 Batch 100 Loss 0.7869 Accuracy 0.2989\n",
            "Epoch 18 Batch 150 Loss 0.7028 Accuracy 0.3343\n",
            "Epoch 18 Batch 200 Loss 0.6202 Accuracy 0.3636\n",
            "Epoch 18 Batch 250 Loss 0.5537 Accuracy 0.3849\n",
            "Epoch 18 Batch 300 Loss 0.5088 Accuracy 0.3998\n",
            "Epoch 18 Loss 0.5014 Accuracy 0.4023\n",
            "Epoch 19 Batch 0 Loss 0.1636 Accuracy 0.4844\n",
            "Epoch 19 Batch 50 Loss 0.3126 Accuracy 0.4691\n",
            "Epoch 19 Batch 100 Loss 0.2870 Accuracy 0.4715\n",
            "Epoch 19 Batch 150 Loss 0.2681 Accuracy 0.4742\n",
            "Epoch 19 Batch 200 Loss 0.2633 Accuracy 0.4753\n",
            "Epoch 19 Batch 250 Loss 0.2613 Accuracy 0.4760\n",
            "Epoch 19 Batch 300 Loss 0.2671 Accuracy 0.4754\n",
            "Epoch 19 Loss 0.2659 Accuracy 0.4756\n",
            "Epoch 20 Batch 0 Loss 0.0591 Accuracy 0.5000\n",
            "Epoch 20 Batch 50 Loss 0.1905 Accuracy 0.4861\n",
            "Epoch 20 Batch 100 Loss 0.1619 Accuracy 0.4874\n",
            "Epoch 20 Batch 150 Loss 0.1523 Accuracy 0.4887\n",
            "Epoch 20 Batch 200 Loss 0.1653 Accuracy 0.4865\n",
            "Epoch 20 Batch 250 Loss 0.1531 Accuracy 0.4880\n",
            "Epoch 20 Batch 300 Loss 0.1703 Accuracy 0.4860\n",
            "Epoch 20 Loss 0.1696 Accuracy 0.4862\n",
            "Epoch 21 Batch 0 Loss 0.1354 Accuracy 0.4883\n",
            "Epoch 21 Batch 50 Loss 0.1070 Accuracy 0.4936\n",
            "Epoch 21 Batch 100 Loss 0.1071 Accuracy 0.4936\n",
            "Epoch 21 Batch 150 Loss 0.1285 Accuracy 0.4914\n",
            "Epoch 21 Batch 200 Loss 0.1282 Accuracy 0.4914\n",
            "Epoch 21 Batch 250 Loss 0.1311 Accuracy 0.4915\n",
            "Epoch 21 Batch 300 Loss 0.1306 Accuracy 0.4917\n",
            "Epoch 21 Loss 0.1289 Accuracy 0.4919\n",
            "Epoch 22 Batch 0 Loss 0.1323 Accuracy 0.4883\n",
            "Epoch 22 Batch 50 Loss 0.2343 Accuracy 0.4766\n",
            "Epoch 22 Batch 100 Loss 0.1742 Accuracy 0.4853\n",
            "Epoch 22 Batch 150 Loss 0.1541 Accuracy 0.4878\n",
            "Epoch 22 Batch 200 Loss 0.1431 Accuracy 0.4891\n",
            "Epoch 22 Batch 250 Loss 0.1411 Accuracy 0.4897\n",
            "Epoch 22 Batch 300 Loss 0.1384 Accuracy 0.4901\n",
            "Epoch 22 Loss 0.1369 Accuracy 0.4902\n",
            "Epoch 23 Batch 0 Loss 0.1197 Accuracy 0.4922\n",
            "Epoch 23 Batch 50 Loss 0.0904 Accuracy 0.4933\n",
            "Epoch 23 Batch 100 Loss 0.1198 Accuracy 0.4915\n",
            "Epoch 23 Batch 150 Loss 0.1148 Accuracy 0.4918\n",
            "Epoch 23 Batch 200 Loss 0.1119 Accuracy 0.4920\n",
            "Epoch 23 Batch 250 Loss 0.1130 Accuracy 0.4915\n",
            "Epoch 23 Batch 300 Loss 0.1123 Accuracy 0.4917\n",
            "Epoch 23 Loss 0.1128 Accuracy 0.4917\n",
            "Epoch 24 Batch 0 Loss 0.0928 Accuracy 0.4961\n",
            "Epoch 24 Batch 50 Loss 0.1169 Accuracy 0.4925\n",
            "Epoch 24 Batch 100 Loss 0.1056 Accuracy 0.4930\n",
            "Epoch 24 Batch 150 Loss 0.1113 Accuracy 0.4931\n",
            "Epoch 24 Batch 200 Loss 0.1147 Accuracy 0.4926\n",
            "Epoch 24 Batch 250 Loss 0.1091 Accuracy 0.4929\n",
            "Epoch 24 Batch 300 Loss 0.1065 Accuracy 0.4929\n",
            "Epoch 24 Loss 0.1057 Accuracy 0.4930\n",
            "Epoch 25 Batch 0 Loss 0.0837 Accuracy 0.4844\n",
            "Epoch 25 Batch 50 Loss 0.0890 Accuracy 0.4940\n",
            "Epoch 25 Batch 100 Loss 0.0958 Accuracy 0.4938\n",
            "Epoch 25 Batch 150 Loss 0.0932 Accuracy 0.4937\n",
            "Epoch 25 Batch 200 Loss 0.0903 Accuracy 0.4942\n",
            "Epoch 25 Batch 250 Loss 0.0950 Accuracy 0.4935\n",
            "Epoch 25 Batch 300 Loss 0.0926 Accuracy 0.4938\n",
            "Epoch 25 Loss 0.0919 Accuracy 0.4939\n",
            "Epoch 26 Batch 0 Loss 0.1093 Accuracy 0.5000\n",
            "Epoch 26 Batch 50 Loss 0.1012 Accuracy 0.4937\n",
            "Epoch 26 Batch 100 Loss 0.0904 Accuracy 0.4947\n",
            "Epoch 26 Batch 150 Loss 0.0907 Accuracy 0.4938\n",
            "Epoch 26 Batch 200 Loss 0.0963 Accuracy 0.4930\n",
            "Epoch 26 Batch 250 Loss 0.0921 Accuracy 0.4937\n",
            "Epoch 26 Batch 300 Loss 0.0924 Accuracy 0.4938\n",
            "Epoch 26 Loss 0.0925 Accuracy 0.4937\n",
            "Epoch 27 Batch 0 Loss 0.0189 Accuracy 0.5000\n",
            "Epoch 27 Batch 50 Loss 0.0904 Accuracy 0.4954\n",
            "Epoch 27 Batch 100 Loss 0.0855 Accuracy 0.4956\n",
            "Epoch 27 Batch 150 Loss 0.0832 Accuracy 0.4954\n",
            "Epoch 27 Batch 200 Loss 0.0824 Accuracy 0.4954\n",
            "Epoch 27 Batch 250 Loss 0.0880 Accuracy 0.4947\n",
            "Epoch 27 Batch 300 Loss 0.0878 Accuracy 0.4947\n",
            "Epoch 27 Loss 0.0870 Accuracy 0.4948\n",
            "Epoch 28 Batch 0 Loss 0.1081 Accuracy 0.4883\n",
            "Epoch 28 Batch 50 Loss 0.0941 Accuracy 0.4937\n",
            "Epoch 28 Batch 100 Loss 0.0863 Accuracy 0.4944\n",
            "Epoch 28 Batch 150 Loss 0.0854 Accuracy 0.4945\n",
            "Epoch 28 Batch 200 Loss 0.0861 Accuracy 0.4942\n",
            "Epoch 28 Batch 250 Loss 0.0866 Accuracy 0.4946\n",
            "Epoch 28 Batch 300 Loss 0.0859 Accuracy 0.4948\n",
            "Epoch 28 Loss 0.0858 Accuracy 0.4947\n",
            "Epoch 29 Batch 0 Loss 0.0273 Accuracy 0.5039\n",
            "Epoch 29 Batch 50 Loss 0.0717 Accuracy 0.4956\n",
            "Epoch 29 Batch 100 Loss 0.0750 Accuracy 0.4951\n",
            "Epoch 29 Batch 150 Loss 0.0772 Accuracy 0.4950\n",
            "Epoch 29 Batch 200 Loss 0.0782 Accuracy 0.4952\n",
            "Epoch 29 Batch 250 Loss 0.0757 Accuracy 0.4955\n",
            "Epoch 29 Batch 300 Loss 0.0753 Accuracy 0.4957\n",
            "Epoch 29 Loss 0.0762 Accuracy 0.4956\n",
            "Epoch 30 Batch 0 Loss 0.1149 Accuracy 0.4922\n",
            "Epoch 30 Batch 50 Loss 0.0676 Accuracy 0.4946\n",
            "Epoch 30 Batch 100 Loss 0.0666 Accuracy 0.4959\n",
            "Epoch 30 Batch 150 Loss 0.0694 Accuracy 0.4961\n",
            "Epoch 30 Batch 200 Loss 0.0715 Accuracy 0.4960\n",
            "Epoch 30 Batch 250 Loss 0.0742 Accuracy 0.4957\n",
            "Epoch 30 Batch 300 Loss 0.0745 Accuracy 0.4955\n",
            "Epoch 30 Loss 0.0749 Accuracy 0.4956\n",
            "Epoch 31 Batch 0 Loss 0.0731 Accuracy 0.4961\n",
            "Epoch 31 Batch 50 Loss 0.0699 Accuracy 0.4969\n",
            "Epoch 31 Batch 100 Loss 0.0756 Accuracy 0.4961\n",
            "Epoch 31 Batch 150 Loss 0.0746 Accuracy 0.4960\n",
            "Epoch 31 Batch 200 Loss 0.0746 Accuracy 0.4961\n",
            "Epoch 31 Batch 250 Loss 0.0785 Accuracy 0.4955\n",
            "Epoch 31 Batch 300 Loss 0.0767 Accuracy 0.4955\n",
            "Epoch 31 Loss 0.0763 Accuracy 0.4955\n",
            "Epoch 32 Batch 0 Loss 0.0872 Accuracy 0.5000\n",
            "Epoch 32 Batch 50 Loss 0.0703 Accuracy 0.4975\n",
            "Epoch 32 Batch 100 Loss 0.0702 Accuracy 0.4968\n",
            "Epoch 32 Batch 150 Loss 0.0729 Accuracy 0.4968\n",
            "Epoch 32 Batch 200 Loss 0.0717 Accuracy 0.4965\n",
            "Epoch 32 Batch 250 Loss 0.0692 Accuracy 0.4966\n",
            "Epoch 32 Batch 300 Loss 0.0693 Accuracy 0.4966\n",
            "Epoch 32 Loss 0.0703 Accuracy 0.4966\n",
            "Epoch 33 Batch 0 Loss 0.0078 Accuracy 0.5000\n",
            "Epoch 33 Batch 50 Loss 0.0760 Accuracy 0.4971\n",
            "Epoch 33 Batch 100 Loss 0.0778 Accuracy 0.4962\n",
            "Epoch 33 Batch 150 Loss 0.0778 Accuracy 0.4960\n",
            "Epoch 33 Batch 200 Loss 0.0741 Accuracy 0.4964\n",
            "Epoch 33 Batch 250 Loss 0.0733 Accuracy 0.4964\n",
            "Epoch 33 Batch 300 Loss 0.0704 Accuracy 0.4966\n",
            "Epoch 33 Loss 0.0700 Accuracy 0.4966\n",
            "Epoch 34 Batch 0 Loss 0.0662 Accuracy 0.5000\n",
            "Epoch 34 Batch 50 Loss 0.0652 Accuracy 0.4975\n",
            "Epoch 34 Batch 100 Loss 0.0667 Accuracy 0.4976\n",
            "Epoch 34 Batch 150 Loss 0.0661 Accuracy 0.4978\n",
            "Epoch 34 Batch 200 Loss 0.0649 Accuracy 0.4975\n",
            "Epoch 34 Batch 250 Loss 0.0657 Accuracy 0.4969\n",
            "Epoch 34 Batch 300 Loss 0.0686 Accuracy 0.4967\n",
            "Epoch 34 Loss 0.0683 Accuracy 0.4967\n",
            "Epoch 35 Batch 0 Loss 0.0538 Accuracy 0.4922\n",
            "Epoch 35 Batch 50 Loss 0.0534 Accuracy 0.4977\n",
            "Epoch 35 Batch 100 Loss 0.0626 Accuracy 0.4969\n",
            "Epoch 35 Batch 150 Loss 0.0637 Accuracy 0.4969\n",
            "Epoch 35 Batch 200 Loss 0.0650 Accuracy 0.4971\n",
            "Epoch 35 Batch 250 Loss 0.0656 Accuracy 0.4969\n",
            "Epoch 35 Batch 300 Loss 0.0643 Accuracy 0.4971\n",
            "Epoch 35 Loss 0.0657 Accuracy 0.4970\n",
            "Epoch 36 Batch 0 Loss 0.0831 Accuracy 0.4922\n",
            "Epoch 36 Batch 50 Loss 0.0684 Accuracy 0.4972\n",
            "Epoch 36 Batch 100 Loss 0.0672 Accuracy 0.4975\n",
            "Epoch 36 Batch 150 Loss 0.0677 Accuracy 0.4972\n",
            "Epoch 36 Batch 200 Loss 0.0656 Accuracy 0.4974\n",
            "Epoch 36 Batch 250 Loss 0.0654 Accuracy 0.4974\n",
            "Epoch 36 Batch 300 Loss 0.0643 Accuracy 0.4973\n",
            "Epoch 36 Loss 0.0640 Accuracy 0.4974\n",
            "Epoch 37 Batch 0 Loss 0.0816 Accuracy 0.5000\n",
            "Epoch 37 Batch 50 Loss 0.0490 Accuracy 0.4975\n",
            "Epoch 37 Batch 100 Loss 0.0539 Accuracy 0.4976\n",
            "Epoch 37 Batch 150 Loss 0.0583 Accuracy 0.4979\n",
            "Epoch 37 Batch 200 Loss 0.0629 Accuracy 0.4977\n",
            "Epoch 37 Batch 250 Loss 0.0643 Accuracy 0.4975\n",
            "Epoch 37 Batch 300 Loss 0.0638 Accuracy 0.4976\n",
            "Epoch 37 Loss 0.0634 Accuracy 0.4976\n",
            "Epoch 38 Batch 0 Loss 0.0192 Accuracy 0.4961\n",
            "Epoch 38 Batch 50 Loss 0.0623 Accuracy 0.4983\n",
            "Epoch 38 Batch 100 Loss 0.0689 Accuracy 0.4976\n",
            "Epoch 38 Batch 150 Loss 0.0657 Accuracy 0.4976\n",
            "Epoch 38 Batch 200 Loss 0.0652 Accuracy 0.4976\n",
            "Epoch 38 Batch 250 Loss 0.0660 Accuracy 0.4976\n",
            "Epoch 38 Batch 300 Loss 0.0639 Accuracy 0.4976\n",
            "Epoch 38 Loss 0.0634 Accuracy 0.4976\n",
            "Epoch 39 Batch 0 Loss 0.1024 Accuracy 0.4844\n",
            "Epoch 39 Batch 50 Loss 0.0579 Accuracy 0.4969\n",
            "Epoch 39 Batch 100 Loss 0.0575 Accuracy 0.4970\n",
            "Epoch 39 Batch 150 Loss 0.0559 Accuracy 0.4977\n",
            "Epoch 39 Batch 200 Loss 0.0586 Accuracy 0.4978\n",
            "Epoch 39 Batch 250 Loss 0.0606 Accuracy 0.4974\n",
            "Epoch 39 Batch 300 Loss 0.0622 Accuracy 0.4973\n",
            "Epoch 39 Loss 0.0625 Accuracy 0.4973\n",
            "Epoch 40 Batch 0 Loss 0.0231 Accuracy 0.5039\n",
            "Epoch 40 Batch 50 Loss 0.0541 Accuracy 0.4968\n",
            "Epoch 40 Batch 100 Loss 0.0569 Accuracy 0.4974\n",
            "Epoch 40 Batch 150 Loss 0.0555 Accuracy 0.4977\n",
            "Epoch 40 Batch 200 Loss 0.0597 Accuracy 0.4974\n",
            "Epoch 40 Batch 250 Loss 0.0605 Accuracy 0.4972\n",
            "Epoch 40 Batch 300 Loss 0.0599 Accuracy 0.4977\n",
            "Epoch 40 Loss 0.0608 Accuracy 0.4976\n",
            "Epoch 41 Batch 0 Loss 0.0049 Accuracy 0.5000\n",
            "Epoch 41 Batch 50 Loss 0.0617 Accuracy 0.4979\n",
            "Epoch 41 Batch 100 Loss 0.0651 Accuracy 0.4974\n",
            "Epoch 41 Batch 150 Loss 0.0613 Accuracy 0.4976\n",
            "Epoch 41 Batch 200 Loss 0.0598 Accuracy 0.4978\n",
            "Epoch 41 Batch 250 Loss 0.0611 Accuracy 0.4978\n",
            "Epoch 41 Batch 300 Loss 0.0614 Accuracy 0.4977\n",
            "Epoch 41 Loss 0.0625 Accuracy 0.4977\n",
            "Epoch 42 Batch 0 Loss 0.0740 Accuracy 0.4922\n",
            "Epoch 42 Batch 50 Loss 0.0674 Accuracy 0.4972\n",
            "Epoch 42 Batch 100 Loss 0.0673 Accuracy 0.4972\n",
            "Epoch 42 Batch 150 Loss 0.0659 Accuracy 0.4972\n",
            "Epoch 42 Batch 200 Loss 0.0672 Accuracy 0.4970\n",
            "Epoch 42 Batch 250 Loss 0.0652 Accuracy 0.4971\n",
            "Epoch 42 Batch 300 Loss 0.0629 Accuracy 0.4973\n",
            "Epoch 42 Loss 0.0635 Accuracy 0.4972\n",
            "Epoch 43 Batch 0 Loss 0.1021 Accuracy 0.4961\n",
            "Epoch 43 Batch 50 Loss 0.0711 Accuracy 0.4959\n",
            "Epoch 43 Batch 100 Loss 0.0638 Accuracy 0.4971\n",
            "Epoch 43 Batch 150 Loss 0.0623 Accuracy 0.4970\n",
            "Epoch 43 Batch 200 Loss 0.0621 Accuracy 0.4972\n",
            "Epoch 43 Batch 250 Loss 0.0636 Accuracy 0.4972\n",
            "Epoch 43 Batch 300 Loss 0.0635 Accuracy 0.4972\n",
            "Epoch 43 Loss 0.0631 Accuracy 0.4972\n",
            "Epoch 44 Batch 0 Loss 0.1512 Accuracy 0.4883\n",
            "Epoch 44 Batch 50 Loss 0.0668 Accuracy 0.4966\n",
            "Epoch 44 Batch 100 Loss 0.0634 Accuracy 0.4972\n",
            "Epoch 44 Batch 150 Loss 0.0593 Accuracy 0.4974\n",
            "Epoch 44 Batch 200 Loss 0.0618 Accuracy 0.4976\n",
            "Epoch 44 Batch 250 Loss 0.0616 Accuracy 0.4974\n",
            "Epoch 44 Batch 300 Loss 0.0613 Accuracy 0.4972\n",
            "Epoch 44 Loss 0.0613 Accuracy 0.4973\n",
            "Epoch 45 Batch 0 Loss 0.0189 Accuracy 0.5000\n",
            "Epoch 45 Batch 50 Loss 0.0588 Accuracy 0.4979\n",
            "Epoch 45 Batch 100 Loss 0.0620 Accuracy 0.4976\n",
            "Epoch 45 Batch 150 Loss 0.0626 Accuracy 0.4978\n",
            "Epoch 45 Batch 200 Loss 0.0620 Accuracy 0.4978\n",
            "Epoch 45 Batch 250 Loss 0.0614 Accuracy 0.4980\n",
            "Epoch 45 Batch 300 Loss 0.0620 Accuracy 0.4978\n",
            "Epoch 45 Loss 0.0620 Accuracy 0.4978\n",
            "Epoch 46 Batch 0 Loss 0.0354 Accuracy 0.5039\n",
            "Epoch 46 Batch 50 Loss 0.0557 Accuracy 0.4988\n",
            "Epoch 46 Batch 100 Loss 0.0565 Accuracy 0.4980\n",
            "Epoch 46 Batch 150 Loss 0.0543 Accuracy 0.4983\n",
            "Epoch 46 Batch 200 Loss 0.0558 Accuracy 0.4981\n",
            "Epoch 46 Batch 250 Loss 0.0575 Accuracy 0.4981\n",
            "Epoch 46 Batch 300 Loss 0.0586 Accuracy 0.4980\n",
            "Epoch 46 Loss 0.0583 Accuracy 0.4980\n",
            "Epoch 47 Batch 0 Loss 0.0342 Accuracy 0.4961\n",
            "Epoch 47 Batch 50 Loss 0.0541 Accuracy 0.4991\n",
            "Epoch 47 Batch 100 Loss 0.0495 Accuracy 0.4992\n",
            "Epoch 47 Batch 150 Loss 0.0535 Accuracy 0.4985\n",
            "Epoch 47 Batch 200 Loss 0.0557 Accuracy 0.4983\n",
            "Epoch 47 Batch 250 Loss 0.0575 Accuracy 0.4982\n",
            "Epoch 47 Batch 300 Loss 0.0594 Accuracy 0.4981\n",
            "Epoch 47 Loss 0.0591 Accuracy 0.4980\n",
            "Epoch 48 Batch 0 Loss 0.0034 Accuracy 0.5000\n",
            "Epoch 48 Batch 50 Loss 0.0586 Accuracy 0.4979\n",
            "Epoch 48 Batch 100 Loss 0.0551 Accuracy 0.4984\n",
            "Epoch 48 Batch 150 Loss 0.0555 Accuracy 0.4980\n",
            "Epoch 48 Batch 200 Loss 0.0554 Accuracy 0.4980\n",
            "Epoch 48 Batch 250 Loss 0.0556 Accuracy 0.4982\n",
            "Epoch 48 Batch 300 Loss 0.0576 Accuracy 0.4981\n",
            "Epoch 48 Loss 0.0583 Accuracy 0.4980\n",
            "Epoch 49 Batch 0 Loss 0.0908 Accuracy 0.4922\n",
            "Epoch 49 Batch 50 Loss 0.0609 Accuracy 0.4989\n",
            "Epoch 49 Batch 100 Loss 0.0584 Accuracy 0.4985\n",
            "Epoch 49 Batch 150 Loss 0.0610 Accuracy 0.4983\n",
            "Epoch 49 Batch 200 Loss 0.0586 Accuracy 0.4982\n",
            "Epoch 49 Batch 250 Loss 0.0584 Accuracy 0.4984\n",
            "Epoch 49 Batch 300 Loss 0.0569 Accuracy 0.4983\n",
            "Epoch 49 Loss 0.0565 Accuracy 0.4983\n",
            "Epoch 50 Batch 0 Loss 0.0176 Accuracy 0.5000\n",
            "Epoch 50 Batch 50 Loss 0.0491 Accuracy 0.4983\n",
            "Epoch 50 Batch 100 Loss 0.0565 Accuracy 0.4988\n",
            "Epoch 50 Batch 150 Loss 0.0555 Accuracy 0.4988\n",
            "Epoch 50 Batch 200 Loss 0.0553 Accuracy 0.4987\n",
            "Epoch 50 Batch 250 Loss 0.0566 Accuracy 0.4987\n",
            "Epoch 50 Batch 300 Loss 0.0582 Accuracy 0.4985\n",
            "Epoch 50 Loss 0.0574 Accuracy 0.4985\n",
            "Epoch 51 Batch 0 Loss 0.0867 Accuracy 0.4844\n",
            "Epoch 51 Batch 50 Loss 0.0644 Accuracy 0.4975\n",
            "Epoch 51 Batch 100 Loss 0.0573 Accuracy 0.4976\n",
            "Epoch 51 Batch 150 Loss 0.0544 Accuracy 0.4979\n",
            "Epoch 51 Batch 200 Loss 0.0544 Accuracy 0.4983\n",
            "Epoch 51 Batch 250 Loss 0.0560 Accuracy 0.4982\n",
            "Epoch 51 Batch 300 Loss 0.0568 Accuracy 0.4983\n",
            "Epoch 51 Loss 0.0568 Accuracy 0.4983\n",
            "Epoch 52 Batch 0 Loss 0.0123 Accuracy 0.4961\n",
            "Epoch 52 Batch 50 Loss 0.0459 Accuracy 0.4989\n",
            "Epoch 52 Batch 100 Loss 0.0481 Accuracy 0.4985\n",
            "Epoch 52 Batch 150 Loss 0.0541 Accuracy 0.4985\n",
            "Epoch 52 Batch 200 Loss 0.0567 Accuracy 0.4984\n",
            "Epoch 52 Batch 250 Loss 0.0553 Accuracy 0.4988\n",
            "Epoch 52 Batch 300 Loss 0.0542 Accuracy 0.4988\n",
            "Epoch 52 Loss 0.0544 Accuracy 0.4988\n",
            "Epoch 53 Batch 0 Loss 0.1209 Accuracy 0.4922\n",
            "Epoch 53 Batch 50 Loss 0.0495 Accuracy 0.4989\n",
            "Epoch 53 Batch 100 Loss 0.0583 Accuracy 0.4986\n",
            "Epoch 53 Batch 150 Loss 0.0526 Accuracy 0.4985\n",
            "Epoch 53 Batch 200 Loss 0.0539 Accuracy 0.4984\n",
            "Epoch 53 Batch 250 Loss 0.0570 Accuracy 0.4985\n",
            "Epoch 53 Batch 300 Loss 0.0580 Accuracy 0.4983\n",
            "Epoch 53 Loss 0.0577 Accuracy 0.4983\n",
            "Epoch 54 Batch 0 Loss 0.1199 Accuracy 0.4883\n",
            "Epoch 54 Batch 50 Loss 0.0508 Accuracy 0.4985\n",
            "Epoch 54 Batch 100 Loss 0.0522 Accuracy 0.4990\n",
            "Epoch 54 Batch 150 Loss 0.0543 Accuracy 0.4988\n",
            "Epoch 54 Batch 200 Loss 0.0559 Accuracy 0.4987\n",
            "Epoch 54 Batch 250 Loss 0.0542 Accuracy 0.4988\n",
            "Epoch 54 Batch 300 Loss 0.0560 Accuracy 0.4985\n",
            "Epoch 54 Loss 0.0564 Accuracy 0.4984\n",
            "Epoch 55 Batch 0 Loss 0.0834 Accuracy 0.4922\n",
            "Epoch 55 Batch 50 Loss 0.0437 Accuracy 0.4990\n",
            "Epoch 55 Batch 100 Loss 0.0481 Accuracy 0.4989\n",
            "Epoch 55 Batch 150 Loss 0.0495 Accuracy 0.4988\n",
            "Epoch 55 Batch 200 Loss 0.0526 Accuracy 0.4986\n",
            "Epoch 55 Batch 250 Loss 0.0535 Accuracy 0.4986\n",
            "Epoch 55 Batch 300 Loss 0.0548 Accuracy 0.4983\n",
            "Epoch 55 Loss 0.0548 Accuracy 0.4984\n",
            "Epoch 56 Batch 0 Loss 0.0121 Accuracy 0.5039\n",
            "Epoch 56 Batch 50 Loss 0.0515 Accuracy 0.4998\n",
            "Epoch 56 Batch 100 Loss 0.0496 Accuracy 0.4994\n",
            "Epoch 56 Batch 150 Loss 0.0527 Accuracy 0.4992\n",
            "Epoch 56 Batch 200 Loss 0.0538 Accuracy 0.4991\n",
            "Epoch 56 Batch 250 Loss 0.0548 Accuracy 0.4989\n",
            "Epoch 56 Batch 300 Loss 0.0554 Accuracy 0.4988\n",
            "Epoch 56 Loss 0.0554 Accuracy 0.4987\n",
            "Epoch 57 Batch 0 Loss 0.0712 Accuracy 0.4805\n",
            "Epoch 57 Batch 50 Loss 0.0592 Accuracy 0.4971\n",
            "Epoch 57 Batch 100 Loss 0.0595 Accuracy 0.4974\n",
            "Epoch 57 Batch 150 Loss 0.0618 Accuracy 0.4976\n",
            "Epoch 57 Batch 200 Loss 0.0607 Accuracy 0.4979\n",
            "Epoch 57 Batch 250 Loss 0.0592 Accuracy 0.4980\n",
            "Epoch 57 Batch 300 Loss 0.0572 Accuracy 0.4981\n",
            "Epoch 57 Loss 0.0570 Accuracy 0.4981\n",
            "Epoch 58 Batch 0 Loss 0.0367 Accuracy 0.4883\n",
            "Epoch 58 Batch 50 Loss 0.0555 Accuracy 0.4979\n",
            "Epoch 58 Batch 100 Loss 0.0582 Accuracy 0.4979\n",
            "Epoch 58 Batch 150 Loss 0.0566 Accuracy 0.4980\n",
            "Epoch 58 Batch 200 Loss 0.0568 Accuracy 0.4978\n",
            "Epoch 58 Batch 250 Loss 0.0562 Accuracy 0.4977\n",
            "Epoch 58 Batch 300 Loss 0.0565 Accuracy 0.4978\n",
            "Epoch 58 Loss 0.0558 Accuracy 0.4979\n",
            "Epoch 59 Batch 0 Loss 0.0815 Accuracy 0.4961\n",
            "Epoch 59 Batch 50 Loss 0.0537 Accuracy 0.4991\n",
            "Epoch 59 Batch 100 Loss 0.0542 Accuracy 0.4990\n",
            "Epoch 59 Batch 150 Loss 0.0550 Accuracy 0.4989\n",
            "Epoch 59 Batch 200 Loss 0.0545 Accuracy 0.4986\n",
            "Epoch 59 Batch 250 Loss 0.0531 Accuracy 0.4988\n",
            "Epoch 59 Batch 300 Loss 0.0553 Accuracy 0.4987\n",
            "Epoch 59 Loss 0.0556 Accuracy 0.4987\n",
            "Epoch 60 Batch 0 Loss 0.0059 Accuracy 0.5000\n",
            "Epoch 60 Batch 50 Loss 0.0585 Accuracy 0.4992\n",
            "Epoch 60 Batch 100 Loss 0.0533 Accuracy 0.4994\n",
            "Epoch 60 Batch 150 Loss 0.0540 Accuracy 0.4991\n",
            "Epoch 60 Batch 200 Loss 0.0554 Accuracy 0.4992\n",
            "Epoch 60 Batch 250 Loss 0.0562 Accuracy 0.4989\n",
            "Epoch 60 Batch 300 Loss 0.0549 Accuracy 0.4987\n",
            "Epoch 60 Loss 0.0546 Accuracy 0.4987\n",
            "Epoch 61 Batch 0 Loss 0.0723 Accuracy 0.4922\n",
            "Epoch 61 Batch 50 Loss 0.0446 Accuracy 0.4992\n",
            "Epoch 61 Batch 100 Loss 0.0574 Accuracy 0.4984\n",
            "Epoch 61 Batch 150 Loss 0.0531 Accuracy 0.4989\n",
            "Epoch 61 Batch 200 Loss 0.0537 Accuracy 0.4989\n",
            "Epoch 61 Batch 250 Loss 0.0542 Accuracy 0.4989\n",
            "Epoch 61 Batch 300 Loss 0.0557 Accuracy 0.4987\n",
            "Epoch 61 Loss 0.0555 Accuracy 0.4987\n",
            "Epoch 62 Batch 0 Loss 0.0318 Accuracy 0.4961\n",
            "Epoch 62 Batch 50 Loss 0.0469 Accuracy 0.4992\n",
            "Epoch 62 Batch 100 Loss 0.0514 Accuracy 0.4992\n",
            "Epoch 62 Batch 150 Loss 0.0550 Accuracy 0.4992\n",
            "Epoch 62 Batch 200 Loss 0.0538 Accuracy 0.4989\n",
            "Epoch 62 Batch 250 Loss 0.0549 Accuracy 0.4987\n",
            "Epoch 62 Batch 300 Loss 0.0529 Accuracy 0.4988\n",
            "Epoch 62 Loss 0.0528 Accuracy 0.4988\n",
            "Epoch 63 Batch 0 Loss 0.1619 Accuracy 0.5000\n",
            "Epoch 63 Batch 50 Loss 0.0521 Accuracy 0.4991\n",
            "Epoch 63 Batch 100 Loss 0.0572 Accuracy 0.4991\n",
            "Epoch 63 Batch 150 Loss 0.0576 Accuracy 0.4989\n",
            "Epoch 63 Batch 200 Loss 0.0570 Accuracy 0.4990\n",
            "Epoch 63 Batch 250 Loss 0.0564 Accuracy 0.4987\n",
            "Epoch 63 Batch 300 Loss 0.0553 Accuracy 0.4990\n",
            "Epoch 63 Loss 0.0544 Accuracy 0.4991\n",
            "Epoch 64 Batch 0 Loss 0.0094 Accuracy 0.5000\n",
            "Epoch 64 Batch 50 Loss 0.0578 Accuracy 0.4993\n",
            "Epoch 64 Batch 100 Loss 0.0507 Accuracy 0.4995\n",
            "Epoch 64 Batch 150 Loss 0.0489 Accuracy 0.4993\n",
            "Epoch 64 Batch 200 Loss 0.0525 Accuracy 0.4988\n",
            "Epoch 64 Batch 250 Loss 0.0527 Accuracy 0.4988\n",
            "Epoch 64 Batch 300 Loss 0.0532 Accuracy 0.4990\n",
            "Epoch 64 Loss 0.0527 Accuracy 0.4990\n",
            "Epoch 65 Batch 0 Loss 0.0557 Accuracy 0.4961\n",
            "Epoch 65 Batch 50 Loss 0.0569 Accuracy 0.4985\n",
            "Epoch 65 Batch 100 Loss 0.0530 Accuracy 0.4991\n",
            "Epoch 65 Batch 150 Loss 0.0554 Accuracy 0.4988\n",
            "Epoch 65 Batch 200 Loss 0.0543 Accuracy 0.4991\n",
            "Epoch 65 Batch 250 Loss 0.0544 Accuracy 0.4990\n",
            "Epoch 65 Batch 300 Loss 0.0558 Accuracy 0.4989\n",
            "Epoch 65 Loss 0.0550 Accuracy 0.4989\n",
            "Epoch 66 Batch 0 Loss 0.0086 Accuracy 0.5039\n",
            "Epoch 66 Batch 50 Loss 0.0479 Accuracy 0.4998\n",
            "Epoch 66 Batch 100 Loss 0.0518 Accuracy 0.4986\n",
            "Epoch 66 Batch 150 Loss 0.0505 Accuracy 0.4988\n",
            "Epoch 66 Batch 200 Loss 0.0531 Accuracy 0.4985\n",
            "Epoch 66 Batch 250 Loss 0.0549 Accuracy 0.4985\n",
            "Epoch 66 Batch 300 Loss 0.0563 Accuracy 0.4983\n",
            "Epoch 66 Loss 0.0567 Accuracy 0.4983\n",
            "Epoch 67 Batch 0 Loss 0.0846 Accuracy 0.5000\n",
            "Epoch 67 Batch 50 Loss 0.0559 Accuracy 0.4981\n",
            "Epoch 67 Batch 100 Loss 0.0525 Accuracy 0.4985\n",
            "Epoch 67 Batch 150 Loss 0.0577 Accuracy 0.4985\n",
            "Epoch 67 Batch 200 Loss 0.0557 Accuracy 0.4989\n",
            "Epoch 67 Batch 250 Loss 0.0538 Accuracy 0.4991\n",
            "Epoch 67 Batch 300 Loss 0.0539 Accuracy 0.4990\n",
            "Epoch 67 Loss 0.0543 Accuracy 0.4989\n",
            "Epoch 68 Batch 0 Loss 0.0968 Accuracy 0.5039\n",
            "Epoch 68 Batch 50 Loss 0.0534 Accuracy 0.4995\n",
            "Epoch 68 Batch 100 Loss 0.0528 Accuracy 0.4994\n",
            "Epoch 68 Batch 150 Loss 0.0534 Accuracy 0.4992\n",
            "Epoch 68 Batch 200 Loss 0.0515 Accuracy 0.4991\n",
            "Epoch 68 Batch 250 Loss 0.0523 Accuracy 0.4993\n",
            "Epoch 68 Batch 300 Loss 0.0520 Accuracy 0.4991\n",
            "Epoch 68 Loss 0.0516 Accuracy 0.4991\n",
            "Epoch 69 Batch 0 Loss 0.0168 Accuracy 0.5039\n",
            "Epoch 69 Batch 50 Loss 0.0488 Accuracy 0.5002\n",
            "Epoch 69 Batch 100 Loss 0.0470 Accuracy 0.4996\n",
            "Epoch 69 Batch 150 Loss 0.0484 Accuracy 0.4994\n",
            "Epoch 69 Batch 200 Loss 0.0513 Accuracy 0.4990\n",
            "Epoch 69 Batch 250 Loss 0.0510 Accuracy 0.4990\n",
            "Epoch 69 Batch 300 Loss 0.0502 Accuracy 0.4991\n",
            "Epoch 69 Loss 0.0501 Accuracy 0.4993\n",
            "Epoch 70 Batch 0 Loss 0.0073 Accuracy 0.5000\n",
            "Epoch 70 Batch 50 Loss 0.0744 Accuracy 0.4975\n",
            "Epoch 70 Batch 100 Loss 0.0639 Accuracy 0.4980\n",
            "Epoch 70 Batch 150 Loss 0.0559 Accuracy 0.4986\n",
            "Epoch 70 Batch 200 Loss 0.0543 Accuracy 0.4988\n",
            "Epoch 70 Batch 250 Loss 0.0537 Accuracy 0.4989\n",
            "Epoch 70 Batch 300 Loss 0.0529 Accuracy 0.4990\n",
            "Epoch 70 Loss 0.0526 Accuracy 0.4990\n",
            "Epoch 71 Batch 0 Loss 0.0802 Accuracy 0.5000\n",
            "Epoch 71 Batch 50 Loss 0.0461 Accuracy 0.4992\n",
            "Epoch 71 Batch 100 Loss 0.0520 Accuracy 0.4988\n",
            "Epoch 71 Batch 150 Loss 0.0531 Accuracy 0.4987\n",
            "Epoch 71 Batch 200 Loss 0.0523 Accuracy 0.4986\n",
            "Epoch 71 Batch 250 Loss 0.0521 Accuracy 0.4987\n",
            "Epoch 71 Batch 300 Loss 0.0514 Accuracy 0.4988\n",
            "Epoch 71 Loss 0.0521 Accuracy 0.4988\n",
            "Epoch 72 Batch 0 Loss 0.0332 Accuracy 0.5000\n",
            "Epoch 72 Batch 50 Loss 0.0455 Accuracy 0.4996\n",
            "Epoch 72 Batch 100 Loss 0.0496 Accuracy 0.4998\n",
            "Epoch 72 Batch 150 Loss 0.0503 Accuracy 0.4992\n",
            "Epoch 72 Batch 200 Loss 0.0489 Accuracy 0.4994\n",
            "Epoch 72 Batch 250 Loss 0.0519 Accuracy 0.4993\n",
            "Epoch 72 Batch 300 Loss 0.0527 Accuracy 0.4990\n",
            "Epoch 72 Loss 0.0527 Accuracy 0.4990\n",
            "Epoch 73 Batch 0 Loss 0.0376 Accuracy 0.5117\n",
            "Epoch 73 Batch 50 Loss 0.0513 Accuracy 0.5004\n",
            "Epoch 73 Batch 100 Loss 0.0488 Accuracy 0.5000\n",
            "Epoch 73 Batch 150 Loss 0.0500 Accuracy 0.4995\n",
            "Epoch 73 Batch 200 Loss 0.0516 Accuracy 0.4991\n",
            "Epoch 73 Batch 250 Loss 0.0531 Accuracy 0.4988\n",
            "Epoch 73 Batch 300 Loss 0.0532 Accuracy 0.4987\n",
            "Epoch 73 Loss 0.0523 Accuracy 0.4988\n",
            "Epoch 74 Batch 0 Loss 0.0254 Accuracy 0.5078\n",
            "Epoch 74 Batch 50 Loss 0.0436 Accuracy 0.4988\n",
            "Epoch 74 Batch 100 Loss 0.0458 Accuracy 0.4986\n",
            "Epoch 74 Batch 150 Loss 0.0503 Accuracy 0.4988\n",
            "Epoch 74 Batch 200 Loss 0.0495 Accuracy 0.4986\n",
            "Epoch 74 Batch 250 Loss 0.0515 Accuracy 0.4988\n",
            "Epoch 74 Batch 300 Loss 0.0500 Accuracy 0.4987\n",
            "Epoch 74 Loss 0.0502 Accuracy 0.4987\n",
            "Epoch 75 Batch 0 Loss 0.0631 Accuracy 0.5000\n",
            "Epoch 75 Batch 50 Loss 0.0581 Accuracy 0.4990\n",
            "Epoch 75 Batch 100 Loss 0.0477 Accuracy 0.4997\n",
            "Epoch 75 Batch 150 Loss 0.0480 Accuracy 0.4997\n",
            "Epoch 75 Batch 200 Loss 0.0478 Accuracy 0.4996\n",
            "Epoch 75 Batch 250 Loss 0.0506 Accuracy 0.4993\n",
            "Epoch 75 Batch 300 Loss 0.0494 Accuracy 0.4994\n",
            "Epoch 75 Loss 0.0497 Accuracy 0.4994\n",
            "Epoch 76 Batch 0 Loss 0.0160 Accuracy 0.5078\n",
            "Epoch 76 Batch 50 Loss 0.0549 Accuracy 0.4988\n",
            "Epoch 76 Batch 100 Loss 0.0565 Accuracy 0.4990\n",
            "Epoch 76 Batch 150 Loss 0.0560 Accuracy 0.4990\n",
            "Epoch 76 Batch 200 Loss 0.0545 Accuracy 0.4992\n",
            "Epoch 76 Batch 250 Loss 0.0546 Accuracy 0.4988\n",
            "Epoch 76 Batch 300 Loss 0.0532 Accuracy 0.4987\n",
            "Epoch 76 Loss 0.0536 Accuracy 0.4987\n",
            "Epoch 77 Batch 0 Loss 0.0202 Accuracy 0.5117\n",
            "Epoch 77 Batch 50 Loss 0.0475 Accuracy 0.4987\n",
            "Epoch 77 Batch 100 Loss 0.0477 Accuracy 0.4990\n",
            "Epoch 77 Batch 150 Loss 0.0482 Accuracy 0.4992\n",
            "Epoch 77 Batch 200 Loss 0.0504 Accuracy 0.4992\n",
            "Epoch 77 Batch 250 Loss 0.0516 Accuracy 0.4992\n",
            "Epoch 77 Batch 300 Loss 0.0507 Accuracy 0.4992\n",
            "Epoch 77 Loss 0.0512 Accuracy 0.4992\n",
            "Epoch 78 Batch 0 Loss 0.0129 Accuracy 0.4961\n",
            "Epoch 78 Batch 50 Loss 0.0533 Accuracy 0.4984\n",
            "Epoch 78 Batch 100 Loss 0.0505 Accuracy 0.4982\n",
            "Epoch 78 Batch 150 Loss 0.0535 Accuracy 0.4984\n",
            "Epoch 78 Batch 200 Loss 0.0518 Accuracy 0.4988\n",
            "Epoch 78 Batch 250 Loss 0.0524 Accuracy 0.4988\n",
            "Epoch 78 Batch 300 Loss 0.0507 Accuracy 0.4988\n",
            "Epoch 78 Loss 0.0507 Accuracy 0.4989\n",
            "Epoch 79 Batch 0 Loss 0.0635 Accuracy 0.4961\n",
            "Epoch 79 Batch 50 Loss 0.0524 Accuracy 0.4986\n",
            "Epoch 79 Batch 100 Loss 0.0493 Accuracy 0.4995\n",
            "Epoch 79 Batch 150 Loss 0.0497 Accuracy 0.4991\n",
            "Epoch 79 Batch 200 Loss 0.0509 Accuracy 0.4990\n",
            "Epoch 79 Batch 250 Loss 0.0514 Accuracy 0.4990\n",
            "Epoch 79 Batch 300 Loss 0.0526 Accuracy 0.4990\n",
            "Epoch 79 Loss 0.0520 Accuracy 0.4991\n",
            "Epoch 80 Batch 0 Loss 0.0662 Accuracy 0.5039\n",
            "Epoch 80 Batch 50 Loss 0.0571 Accuracy 0.4988\n",
            "Epoch 80 Batch 100 Loss 0.0503 Accuracy 0.4989\n",
            "Epoch 80 Batch 150 Loss 0.0496 Accuracy 0.4987\n",
            "Epoch 80 Batch 200 Loss 0.0504 Accuracy 0.4988\n",
            "Epoch 80 Batch 250 Loss 0.0518 Accuracy 0.4987\n",
            "Epoch 80 Batch 300 Loss 0.0506 Accuracy 0.4988\n",
            "Epoch 80 Loss 0.0509 Accuracy 0.4988\n",
            "Epoch 81 Batch 0 Loss 0.1970 Accuracy 0.4844\n",
            "Epoch 81 Batch 50 Loss 0.0505 Accuracy 0.4992\n",
            "Epoch 81 Batch 100 Loss 0.0458 Accuracy 0.4991\n",
            "Epoch 81 Batch 150 Loss 0.0524 Accuracy 0.4991\n",
            "Epoch 81 Batch 200 Loss 0.0543 Accuracy 0.4989\n",
            "Epoch 81 Batch 250 Loss 0.0535 Accuracy 0.4988\n",
            "Epoch 81 Batch 300 Loss 0.0526 Accuracy 0.4989\n",
            "Epoch 81 Loss 0.0529 Accuracy 0.4989\n",
            "Epoch 82 Batch 0 Loss 0.0984 Accuracy 0.4961\n",
            "Epoch 82 Batch 50 Loss 0.0418 Accuracy 0.4993\n",
            "Epoch 82 Batch 100 Loss 0.0422 Accuracy 0.4996\n",
            "Epoch 82 Batch 150 Loss 0.0440 Accuracy 0.4993\n",
            "Epoch 82 Batch 200 Loss 0.0449 Accuracy 0.4990\n",
            "Epoch 82 Batch 250 Loss 0.0469 Accuracy 0.4989\n",
            "Epoch 82 Batch 300 Loss 0.0487 Accuracy 0.4989\n",
            "Epoch 82 Loss 0.0486 Accuracy 0.4990\n",
            "Epoch 83 Batch 0 Loss 0.0484 Accuracy 0.5000\n",
            "Epoch 83 Batch 50 Loss 0.0540 Accuracy 0.4996\n",
            "Epoch 83 Batch 100 Loss 0.0534 Accuracy 0.4995\n",
            "Epoch 83 Batch 150 Loss 0.0540 Accuracy 0.4991\n",
            "Epoch 83 Batch 200 Loss 0.0524 Accuracy 0.4991\n",
            "Epoch 83 Batch 250 Loss 0.0503 Accuracy 0.4994\n",
            "Epoch 83 Batch 300 Loss 0.0509 Accuracy 0.4993\n",
            "Epoch 83 Loss 0.0512 Accuracy 0.4993\n",
            "Epoch 84 Batch 0 Loss 0.0218 Accuracy 0.4961\n",
            "Epoch 84 Batch 50 Loss 0.0506 Accuracy 0.4988\n",
            "Epoch 84 Batch 100 Loss 0.0526 Accuracy 0.4996\n",
            "Epoch 84 Batch 150 Loss 0.0514 Accuracy 0.4995\n",
            "Epoch 84 Batch 200 Loss 0.0507 Accuracy 0.4996\n",
            "Epoch 84 Batch 250 Loss 0.0495 Accuracy 0.4994\n",
            "Epoch 84 Batch 300 Loss 0.0487 Accuracy 0.4993\n",
            "Epoch 84 Loss 0.0496 Accuracy 0.4991\n",
            "Epoch 85 Batch 0 Loss 0.0101 Accuracy 0.5000\n",
            "Epoch 85 Batch 50 Loss 0.0434 Accuracy 0.5004\n",
            "Epoch 85 Batch 100 Loss 0.0427 Accuracy 0.4997\n",
            "Epoch 85 Batch 150 Loss 0.0432 Accuracy 0.4994\n",
            "Epoch 85 Batch 200 Loss 0.0443 Accuracy 0.4992\n",
            "Epoch 85 Batch 250 Loss 0.0439 Accuracy 0.4993\n",
            "Epoch 85 Batch 300 Loss 0.0480 Accuracy 0.4990\n",
            "Epoch 85 Loss 0.0486 Accuracy 0.4991\n",
            "Epoch 86 Batch 0 Loss 0.0558 Accuracy 0.4961\n",
            "Epoch 86 Batch 50 Loss 0.0500 Accuracy 0.4995\n",
            "Epoch 86 Batch 100 Loss 0.0501 Accuracy 0.4992\n",
            "Epoch 86 Batch 150 Loss 0.0539 Accuracy 0.4990\n",
            "Epoch 86 Batch 200 Loss 0.0520 Accuracy 0.4991\n",
            "Epoch 86 Batch 250 Loss 0.0498 Accuracy 0.4994\n",
            "Epoch 86 Batch 300 Loss 0.0490 Accuracy 0.4995\n",
            "Epoch 86 Loss 0.0495 Accuracy 0.4994\n",
            "Epoch 87 Batch 0 Loss 0.0078 Accuracy 0.5039\n",
            "Epoch 87 Batch 50 Loss 0.0520 Accuracy 0.5000\n",
            "Epoch 87 Batch 100 Loss 0.0485 Accuracy 0.4999\n",
            "Epoch 87 Batch 150 Loss 0.0491 Accuracy 0.4997\n",
            "Epoch 87 Batch 200 Loss 0.0492 Accuracy 0.4997\n",
            "Epoch 87 Batch 250 Loss 0.0468 Accuracy 0.4998\n",
            "Epoch 87 Batch 300 Loss 0.0482 Accuracy 0.4998\n",
            "Epoch 87 Loss 0.0487 Accuracy 0.4996\n",
            "Epoch 88 Batch 0 Loss 0.0808 Accuracy 0.5000\n",
            "Epoch 88 Batch 50 Loss 0.0529 Accuracy 0.4990\n",
            "Epoch 88 Batch 100 Loss 0.0548 Accuracy 0.4986\n",
            "Epoch 88 Batch 150 Loss 0.0515 Accuracy 0.4990\n",
            "Epoch 88 Batch 200 Loss 0.0497 Accuracy 0.4993\n",
            "Epoch 88 Batch 250 Loss 0.0506 Accuracy 0.4991\n",
            "Epoch 88 Batch 300 Loss 0.0500 Accuracy 0.4994\n",
            "Epoch 88 Loss 0.0497 Accuracy 0.4993\n",
            "Epoch 89 Batch 0 Loss 0.0423 Accuracy 0.5000\n",
            "Epoch 89 Batch 50 Loss 0.0454 Accuracy 0.5005\n",
            "Epoch 89 Batch 100 Loss 0.0439 Accuracy 0.4994\n",
            "Epoch 89 Batch 150 Loss 0.0452 Accuracy 0.4991\n",
            "Epoch 89 Batch 200 Loss 0.0465 Accuracy 0.4992\n",
            "Epoch 89 Batch 250 Loss 0.0479 Accuracy 0.4991\n",
            "Epoch 89 Batch 300 Loss 0.0489 Accuracy 0.4992\n",
            "Epoch 89 Loss 0.0490 Accuracy 0.4992\n",
            "Epoch 90 Batch 0 Loss 0.0212 Accuracy 0.4961\n",
            "Epoch 90 Batch 50 Loss 0.0455 Accuracy 0.4989\n",
            "Epoch 90 Batch 100 Loss 0.0503 Accuracy 0.4993\n",
            "Epoch 90 Batch 150 Loss 0.0486 Accuracy 0.4990\n",
            "Epoch 90 Batch 200 Loss 0.0505 Accuracy 0.4988\n",
            "Epoch 90 Batch 250 Loss 0.0498 Accuracy 0.4990\n",
            "Epoch 90 Batch 300 Loss 0.0491 Accuracy 0.4991\n",
            "Epoch 90 Loss 0.0488 Accuracy 0.4991\n",
            "Epoch 91 Batch 0 Loss 0.0770 Accuracy 0.5039\n",
            "Epoch 91 Batch 50 Loss 0.0446 Accuracy 0.5011\n",
            "Epoch 91 Batch 100 Loss 0.0401 Accuracy 0.5005\n",
            "Epoch 91 Batch 150 Loss 0.0435 Accuracy 0.5002\n",
            "Epoch 91 Batch 200 Loss 0.0459 Accuracy 0.4999\n",
            "Epoch 91 Batch 250 Loss 0.0473 Accuracy 0.4995\n",
            "Epoch 91 Batch 300 Loss 0.0474 Accuracy 0.4996\n",
            "Epoch 91 Loss 0.0472 Accuracy 0.4996\n",
            "Epoch 92 Batch 0 Loss 0.0547 Accuracy 0.5000\n",
            "Epoch 92 Batch 50 Loss 0.0457 Accuracy 0.5001\n",
            "Epoch 92 Batch 100 Loss 0.0434 Accuracy 0.4999\n",
            "Epoch 92 Batch 150 Loss 0.0453 Accuracy 0.4995\n",
            "Epoch 92 Batch 200 Loss 0.0450 Accuracy 0.4996\n",
            "Epoch 92 Batch 250 Loss 0.0454 Accuracy 0.4993\n",
            "Epoch 92 Batch 300 Loss 0.0464 Accuracy 0.4994\n",
            "Epoch 92 Loss 0.0464 Accuracy 0.4995\n",
            "Epoch 93 Batch 0 Loss 0.0202 Accuracy 0.5039\n",
            "Epoch 93 Batch 50 Loss 0.0498 Accuracy 0.5000\n",
            "Epoch 93 Batch 100 Loss 0.0471 Accuracy 0.4996\n",
            "Epoch 93 Batch 150 Loss 0.0515 Accuracy 0.4990\n",
            "Epoch 93 Batch 200 Loss 0.0514 Accuracy 0.4990\n",
            "Epoch 93 Batch 250 Loss 0.0497 Accuracy 0.4989\n",
            "Epoch 93 Batch 300 Loss 0.0499 Accuracy 0.4991\n",
            "Epoch 93 Loss 0.0499 Accuracy 0.4992\n",
            "Epoch 94 Batch 0 Loss 0.0263 Accuracy 0.5039\n",
            "Epoch 94 Batch 50 Loss 0.0532 Accuracy 0.5000\n",
            "Epoch 94 Batch 100 Loss 0.0519 Accuracy 0.5001\n",
            "Epoch 94 Batch 150 Loss 0.0510 Accuracy 0.4998\n",
            "Epoch 94 Batch 200 Loss 0.0487 Accuracy 0.4997\n",
            "Epoch 94 Batch 250 Loss 0.0500 Accuracy 0.4994\n",
            "Epoch 94 Batch 300 Loss 0.0498 Accuracy 0.4992\n",
            "Epoch 94 Loss 0.0494 Accuracy 0.4992\n",
            "Epoch 95 Batch 0 Loss 0.0545 Accuracy 0.5000\n",
            "Epoch 95 Batch 50 Loss 0.0518 Accuracy 0.4997\n",
            "Epoch 95 Batch 100 Loss 0.0478 Accuracy 0.5000\n",
            "Epoch 95 Batch 150 Loss 0.0470 Accuracy 0.5000\n",
            "Epoch 95 Batch 200 Loss 0.0483 Accuracy 0.4997\n",
            "Epoch 95 Batch 250 Loss 0.0478 Accuracy 0.4996\n",
            "Epoch 95 Batch 300 Loss 0.0490 Accuracy 0.4995\n",
            "Epoch 95 Loss 0.0486 Accuracy 0.4994\n",
            "Epoch 96 Batch 0 Loss 0.0273 Accuracy 0.5039\n",
            "Epoch 96 Batch 50 Loss 0.0513 Accuracy 0.4992\n",
            "Epoch 96 Batch 100 Loss 0.0462 Accuracy 0.4993\n",
            "Epoch 96 Batch 150 Loss 0.0482 Accuracy 0.4995\n",
            "Epoch 96 Batch 200 Loss 0.0466 Accuracy 0.4994\n",
            "Epoch 96 Batch 250 Loss 0.0455 Accuracy 0.4997\n",
            "Epoch 96 Batch 300 Loss 0.0463 Accuracy 0.4995\n",
            "Epoch 96 Loss 0.0465 Accuracy 0.4995\n",
            "Epoch 97 Batch 0 Loss 0.0181 Accuracy 0.4922\n",
            "Epoch 97 Batch 50 Loss 0.0509 Accuracy 0.4993\n",
            "Epoch 97 Batch 100 Loss 0.0457 Accuracy 0.4990\n",
            "Epoch 97 Batch 150 Loss 0.0440 Accuracy 0.4994\n",
            "Epoch 97 Batch 200 Loss 0.0452 Accuracy 0.4995\n",
            "Epoch 97 Batch 250 Loss 0.0469 Accuracy 0.4994\n",
            "Epoch 97 Batch 300 Loss 0.0465 Accuracy 0.4994\n",
            "Epoch 97 Loss 0.0472 Accuracy 0.4993\n",
            "Epoch 98 Batch 0 Loss 0.0162 Accuracy 0.5000\n",
            "Epoch 98 Batch 50 Loss 0.0426 Accuracy 0.4998\n",
            "Epoch 98 Batch 100 Loss 0.0462 Accuracy 0.4998\n",
            "Epoch 98 Batch 150 Loss 0.0478 Accuracy 0.4993\n",
            "Epoch 98 Batch 200 Loss 0.0466 Accuracy 0.4996\n",
            "Epoch 98 Batch 250 Loss 0.0495 Accuracy 0.4992\n",
            "Epoch 98 Batch 300 Loss 0.0476 Accuracy 0.4992\n",
            "Epoch 98 Loss 0.0477 Accuracy 0.4991\n",
            "Epoch 99 Batch 0 Loss 0.1728 Accuracy 0.4844\n",
            "Epoch 99 Batch 50 Loss 0.0514 Accuracy 0.5001\n",
            "Epoch 99 Batch 100 Loss 0.0515 Accuracy 0.4992\n",
            "Epoch 99 Batch 150 Loss 0.0503 Accuracy 0.4991\n",
            "Epoch 99 Batch 200 Loss 0.0498 Accuracy 0.4991\n",
            "Epoch 99 Batch 250 Loss 0.0482 Accuracy 0.4994\n",
            "Epoch 99 Batch 300 Loss 0.0469 Accuracy 0.4995\n",
            "Epoch 99 Loss 0.0465 Accuracy 0.4995\n",
            "Epoch 100 Batch 0 Loss 0.0226 Accuracy 0.4961\n",
            "Epoch 100 Batch 50 Loss 0.0399 Accuracy 0.4998\n",
            "Epoch 100 Batch 100 Loss 0.0428 Accuracy 0.4996\n",
            "Epoch 100 Batch 150 Loss 0.0442 Accuracy 0.4997\n",
            "Epoch 100 Batch 200 Loss 0.0455 Accuracy 0.4997\n",
            "Epoch 100 Batch 250 Loss 0.0480 Accuracy 0.4994\n",
            "Epoch 100 Batch 300 Loss 0.0488 Accuracy 0.4991\n",
            "Epoch 100 Loss 0.0492 Accuracy 0.4990\n",
            "CPU times: user 25min 50s, sys: 2min 8s, total: 27min 58s\n",
            "Wall time: 19min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3O6o6-3c9dq",
        "outputId": "96f94bb1-46da-4312-eb47-8687c3969785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    start_token = [1]\n",
        "    end_token = [2]\n",
        "  \n",
        "    sentence = preprocess_sentence(inp_sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    \n",
        "    encoder_input = tf.expand_dims(inputs, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "    decoder_input = [1]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_length_targ):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "    sentence = inp_lang_tokenizer.encode(sentence)\n",
        "  \n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                            if i < tokenizer_en.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def translate(sentence, plot=''):\n",
        "    result, attention_weights = evaluate(sentence)\n",
        "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
        "        \n",
        "translate(\"good morning.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: good morning.\n",
            "Predicted translation: ['<start>', '.', '.', '.', '.', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGoHtjBTgbUD",
        "outputId": "c10880c9-5abe-4a09-933f-09bf6d920068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "translate(u\"how are you\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: how are you\n",
            "Predicted translation: ['<start>', '.', '.', '.', '.', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPDtecCklzfE"
      },
      "source": [
        "**Вывод:** качество перевода далекое от совершенства, что не удивительно, учитывая примитивность модели, малое количество итераций и слишком маленький объем корпуса параллельных текстов\n",
        "\n",
        "\n"
      ]
    }
  ]
}